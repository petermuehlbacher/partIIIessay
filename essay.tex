\documentclass[11pt,reqno]{amsart}
%\allowdisplaybreaks[4]
\RequirePackage[OT1]{fontenc}
\RequirePackage{amsthm,amsmath}
\RequirePackage[numbers]{natbib}
\RequirePackage[colorlinks,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{amssymb}
\usepackage{anysize}
\usepackage{hyperref}
\usepackage{extarrows}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{stmaryrd}
\usepackage{color}
\usepackage{soul}
\usepackage{todonotes}
%\usepackage{fontspec}
%\usepackage{arydshln}

\numberwithin{equation}{section}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defi}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{assu}[thm]{Assumption}
\newtheorem{Exa}{Example}[section]
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray*}}
\newcommand{\eea}{\end{eqnarray*}}
\renewcommand*{\bibfont}{\footnotesize}


\newcommand{\eqby}[1]{\mathrel{\stackrel{#1}{=}}}
\newcommand{\eqbydef}{\mathrel{\stackrel{\text{(def)}}{=}}}
\newcommand{\leqby}[1]{\mathrel{\stackrel{#1}{\leq}}}
\newcommand{\geqby}[1]{\mathrel{\stackrel{#1}{\geq}}}
\newcommand{\deq}{\mathrel{\mathop:}=}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % use as in http://tex.stackexchange.com/questions/42726/align-but-show-one-equation-number-at-the-end
%\begin{align*}
%a &=b \\
%  &=c \numberthis \label{eqn}
%\end{align*}

\newcommand{\iid}[1]{\mathrel{\stackrel{\text{iid}}{\sim}}#1}
\newcommand{\iidnormal}{\mathrel{\stackrel{\text{iid}}{\sim}}\mathcal N(0,1)}

\newcommand{\mR}{\mathbb R^n}
\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\makeatletter % `@' now normal "letter"
\@addtoreset{equation}{section}
\makeatother  % `@' is restored as "non-letter"
\renewcommand\theequation{{\thesection}%
                   .{\arabic{equation}}}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\abs}[1]{\lvert#1\rvert}

%    Blank box placeholder for figures (to avoid requiring any
%    particular graphics capabilities for printing this document).
\newcommand{\blankbox}[2]{%
  \parbox{\columnwidth}{\centering
%    Set fboxsep to 0 so that the actual size of the box will match the
%    given measurements more closely.
    \setlength{\fboxsep}{0pt}%
    \fbox{\raisebox{0pt}[#2]{\hspace{#1}}}%
  }%
}

\renewcommand{\baselinestretch}{1.2}





\newcommand{\fa}{{\frak a}} 
\newcommand{\bk}{{\bf{k}}}
\newcommand{\ttau}{\vartheta}

\newcommand{\LL}{L}
\newcommand{\fn}{{\mathfrak n}}

\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\const}{\mbox{const}}
\newcommand{\La}{x}
\newcommand{\beqa}{\begin{eqnarray}}
\newcommand{\eeqa}{\end{eqnarray}}
%\newcommand{\e}{\varepsilon}
\newcommand{\eps}{\varepsilon}
\newcommand{\pt}{\partial}
\newcommand{\rd}{{\rm d}}
\newcommand{\bR}{{\mathbb R}}
\newcommand{\bC}{{\mathbb C}}

\newcommand{\bZ}{{\mathbb Z}}
\newcommand{\non}{\nonumber}
\newcommand{\wH}{{K}}


\newcommand{\bke}[1]{\left( #1 \right)}
\newcommand{\bkt}[1]{\left[ #1 \right]}
\newcommand{\bket}[1]{\left\{ #1 \right\}}
\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\Norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\bka}[1]{\left\langle #1 \right\rangle}
\newcommand{\vect}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\K}{n}
\newcommand{\z}{{\underline z}}
\renewcommand{\u}{{\underline u}}

\newcommand{\tr}{\mbox{tr\,}}



\newcommand{\unu}{{\underline {\nu}}}
\newcommand{\umu}{{\underline {\mu}}}


\renewcommand{\Re}{\mathsf{Re}\,}
\renewcommand{\Im}{\mathsf{Im}\,}

\newcommand{\ba}{{\bf{a}}}
\newcommand{\bb}{{\bf{b}}}
\newcommand{\bx}{{\bf{x}}}
\newcommand{\by}{{\bf{y}}}
\newcommand{\bu}{{\bf{u}}}
\newcommand{\bv}{{\bf{v}}}
\newcommand{\bw}{{\bf{w}}}
\newcommand{\bz}{{\bf {z}}}
\newcommand{\bq}{{\bf {q}}}
\newcommand{\tbx}{\widetilde\bx}
\newcommand{\bh}{{\bf{h}}}
\newcommand{\bn}{{\bf{n}}}

\newcommand{\fq}{{\frak q}}
\newcommand{\fu}{{\frak u}}
\newcommand{\bm}{{\bf  m }}
\newcommand{\bmu}{\mbox{\boldmath $\mu$}}
\newcommand{\sa}{{[\alpha ]}}


\newcommand{\tby}{\widetilde\by}

\newcommand{\bT}{{\T}}
\newcommand{\bO}{{\bf O}}

\newcommand{\bla}{\mbox{\boldmath $\lambda$}}
\newcommand{\bnu}{\mbox{\boldmath $\nu$}}
\newcommand{\bbeta}{\mbox{\boldmath $\beta$}}


\newcommand{\wG}{{\widehat G}}
\newcommand{\mg}{{m_H}}
\newcommand{\mW}{{m_W}}

\newcommand{\al}{\alpha}
\newcommand{\de}{\delta}




\newcommand{\barv}{ {[ v ]}}
\newcommand{\barZ}{ {[ Z ]}}

\newcommand{\ga}{{\gamma}}
\newcommand{\Ga}{{\Gamma}}
\newcommand{\la}{\lambda}
\newcommand{\Om}{{\Omega}}
\newcommand{\om}{{\omega}}
\newcommand{\si}{\sigma}
\renewcommand{\th}{\theta}
\newcommand{\td}{\widetilde}
\newcommand{\ze}{\zeta}

\newcommand{\cL}{{\mathscr L}}
\newcommand{\cE}{{\mathcal E}}
\newcommand{\cG}{{\mathcal G}}
\newcommand{\cX}{{\mathcal X}}
\newcommand{\cY}{{\mathcal Y}}
\newcommand{\cK}{{\mathcal K}}




\newcommand{\cM}{{\mathcal M}}
\newcommand{\cN}{{\mathcal N}}
\newcommand{\cH}{{\mathcal H}}
\newcommand{\cB}{{\mathcal B}}
\newcommand{\cU}{{\mathcal U}}

\newcommand{\bW}{{\bf W}}

\newcommand{\ov}{\overline}



%\newcommand{\re}{{\text Re \, }}
\newcommand{\re}{{\mathfrak{Re} \, }}
\newcommand{\im}{{\mathfrak{Im} \, }}
%\newcommand{\im}{{\text Im \, }}
\newcommand{\E}{{\mathbb E }}
\newcommand{\R}{{\mathbb R }}
\newcommand{\N}{{\mathbb N}}


\newcommand{\CC}{{\mathbb C }}
\newcommand{\RR}{{\mathbb R }}
\newcommand{\NN}{{\mathbb N}}
\newcommand{\g}{\gamma}
\newcommand{\ii}{\mathrm{i}}



\newcommand{\bl}{{\boldsymbol \lambda}}


\newcommand{\bt}{{\boldsymbol \theta}}
\newcommand{\htau}{{\hat\tau}}


\newcommand{\Ci}{{ C_{inf}}}
\newcommand{\Cs}{{ C_{sup}}}
\newcommand{\Z}{{\mathbb Z}}
\renewcommand{\P}{{\mathbb P}}


\newcommand{\C}{{\mathbb C}}
\newcommand{\pd}{{\partial}}

\newcommand{\nb}{{\nabla}}
\newcommand{\lec}{\lesssim}
\newcommand{\ind}{{\,\mathrm{d}}}

\renewcommand{\S}{\mathbb S}
\newcommand{\T}{\mathbb T}
\newcommand{\U}{\mathbb U}
\newcommand{\V}{\mathbb V}
\newcommand{\bU}{ {\bf  U}}
\newcommand{\bS}{ {\bf  S}}
\renewcommand{\S}{[\bf S]}

\newcommand{\ph}{{\varphi}}

\renewcommand{\div}{\mathop{\mathrm{div}}}
\newcommand{\curl}{\mathop{\mathrm{curl}}}
\newcommand{\spt}{\mathop{\mathrm{spt}}}
\newcommand{\wkto}{\rightharpoonup}
\newenvironment{pf}{{\bf Proof.}} {\hfill\qed}

\newcommand{\lv}{{\bar v}}
\newcommand{\lp}{{\bar p}}

\newcommand{\Cr}{\color{red}}
\newcommand{\Cb}{\color{blue}}
\newcommand{\Cg}{\color{green}}
\newcommand{\nc}{\normalcolor}

\newcommand{\e}{\varepsilon}


\marginsize{35mm}{35mm}{38mm}{40mm}

\begin{document}

\section*{Notation}
\begin{itemize}
	\item Abbreviate the Gaussian Free Field by GFF.
	\item For $X$ being a random variable on some probability space $(\Omega, \mathcal A, \mathbb P)$ write $\mathcal L(X)$ for the law of a random variable $X$, i.e. the pushforward measure $\mathcal L(X)\deq \mathbb P\circ X^{-1}$.
	\item For a topological space $S$ denote its Borel-$\sigma$-algebra by $\mathcal B_S$.
	\item For a topological vector space $V$ write $V^*$ for its topological/continuous dual space, i.e. the set of all continuous, linear functionals $f:V\rightarrow\mathbb R$.
	\item Unless stated otherwise the probability spaces we are working with will implicitly assumed to be $(\Omega,\mathcal A,\mathbb P)$.
	\item $(f,g)$ for the $L^2$ inner product $\int_D f(x)g(x)dx$ on some space $D$ which is usually clear from context; by abuse of notation (justified by Riesz) we will also write use this notation for more general objects like distributions/linear functionals that are not in $L^2$
	% that's kinda the wrong way round, actually we can define the standard inner product by integration by parts and the inverse Laplacian
	\item $(f,g)_\nabla$ for the Dirichlet inner product $(\nabla f,\nabla g)$; again, by abuse of notation (justified by integration by parts), we will use it also for more general elements $f,g$ for which $(f,g)_\nabla = -(f,\Delta g)$ is well-defined
	\item Write $H(D)$ (or often just $H$ and $H^1(D)$ to highlight it is a Sobolev space) for the Hilbert space completion of $H_s(D)$, the space of smooth functions, compactly supported in $D$, with respect to $(\cdot,\cdot)_\nabla$.
\end{itemize}

\section{Gaussian Free Field}

Prior to defining the GFF we will motivate it with some standard results from Gaussian Processes.

\subsection{Gaussian Processes}

\begin{defi}[Gaussian Process]
	A real valued stochastic process $(X(t):t\in T)$, where $T$ is some index set, whose finite dimensional distributions $\mu_F\deq \mathcal L((X(t):t\in F))$ are multivariate normal on $\mathbb R^F$ for all $F\subseteq T$ finite.
\end{defi}

\begin{rem}
	For historical reasons this is called $T$ like time, but it does not have to be $\mathbb R$. In fact, in what follows we will use a Hilbert space for $T$.
\end{rem}

Given that we did not impose any conditions on $T$ it is natural to ask for conditions under which such a process exists and whether it is unique. To do that we recall the that definition of a covariance on $T$ is a mapping $\Phi:T\times T\rightarrow\mathbb R$ such that we have for any $t_1,\dots, t_n\in T$ that the matrix $(\Phi(t_i,t_j))_{i,j=1}^n$ is symmetric and non-negative definite for all $n\in\mathbb N$.

\begin{lem}[Existence]\label{lem:existenceofGPgivenCov}
	Let $\Phi$ be a covariance on $T$, $f:T\rightarrow\mathbb R$ be a measurable function, then there exists a Gaussian process $(X(t):t\in T)$ such that for all $s,t\in T$ we have
	\begin{itemize}
		\item $\mathbb E X(t) = f(t)$
		\item $\mathbb E[(X(s)-f(s))(X(t)-f(t))]=\Phi(s,t)$
	\end{itemize}
\end{lem}

\todo{check if Kolmogorov's consistency theorem gives the kind of uniqueness we want for the proof in the SLE part}
%\begin{rem}[Uniqueness]
%	Realising $X$
%\end{rem}

\begin{rem}[Identification of Gaussian Processes and Gaussian Random Variables]\label{rem:GPandGrvs}
	It can be shown that for sufficiently nice metric spaces $T$ a Gaussian process $X:\Omega\times T\rightarrow\mathbb R$ naturally induces a Borel random variable $X':(\Omega,\mathcal A)\rightarrow (C_u(T,d),\mathcal B_{(C_u(T,d)})$ in the separable Banach space of $C_u(T,d)$, the space of uniformly continuous (w.r.t. the metric $d$) functions on $T$, by looking at the law of $X'(\omega)\deq X(\omega,\cdot)\in\mathbb R^T$.
	This result formalizes the intuitive notion of a Gaussian process being a ``random function''. The non-trivial parts of the claim are that it can be realised as a \emph{Borel} random variable (which is the natural topology on the target space; a priori Kolmogorov's extension theorem would only give us that elements of the cylinder-$\sigma$-algebra are measurable) and that the target space is \emph{separable}.
\end{rem}

While those assumptions will not hold in our more general setting, it should at least motivate an (informal) correspondence between Gaussian processes and Banach space valued Gaussian random variables as in the following definition, motivated by the characterisation of a Gaussian in $\mathbb R^d$ by its projections on any fixed $w\in\mathbb R^d$ which can, by duality, also be interpreted as a linear functional:

\begin{defi}[Banach Space Valued Gaussian Random Variables]
	For a separable Banach space $(B,\|\cdot\|_B)$ we say that $X:(\Omega,\mathcal A,\mathbb P)\rightarrow(B,\|\cdot\|_B)$ is Gaussian/normal whenever $f(X)$ is normally distributed for all $f\in B^*$.
\end{defi}

In the following our $T$ will not be ``small'' enough for some standard results from Gaussian processes, however, in some sense the probabilistic information about a Gaussian random variable in a separable Banach space is also encoded in its dual process:

\begin{defi}[Dual Process]
	Given some random variable $X$ in a separable Banach space $(B,\|\cdot\|_B)$ define its dual process $(\tilde X(f): f\in B^*)$ by the application $\tilde X(f)\deq f(X)$.
\end{defi}

\begin{rem}
	For example it can be shown that there exists a dense subset $D\subset B^*$ with $\|X\|_B = \sup_{f\in D}|\tilde X(f)|$.
\end{rem}

\begin{rem}
	Note that this dual process $\tilde X$ is actually a Gaussian process (check that its finite dimensional distributions are \emph{jointly} Gaussian by linearity of the functionals).
\end{rem}

Referring to the finite dimensional case again, we would like to generalize the fact that Gaussians in $\mathbb R^d$ can be written as $\sum_{i=1}^d e_ig_i$, where $(e_i)_i$ is any orthonormal basis of $\mathbb R^d$ and $g_i\iidnormal$, for (separable) Banach space valued Gaussian random variables. To do that we have to find a suitable subspace $H$ of the Banach space $B$ and endow it with the right Hilbert space structure to make sense of an orthonormal basis.

It turns out that reproducing kernel Hilbert spaces (henceforth abbreviated by RKHS) are the right choice:

\begin{defi}[Reproducing Kernel Hilbert Space for Gaussian Random Variables]
	Let $X:\Omega\rightarrow B$ be a Gaussian random variable in a separable Banach space $B$, $F\deq\{f(X):f\in B^*\}\subseteq L^2(\mathbb P)$, then the RKHS of $X$ is the Hilbert space $$H\deq\{\phi_h\deq\mathbb E(hX): h\in\overline F^{L^2(\mathbb P)}\},$$
	endowed with the inner product
	$$(\phi_h,\phi_g)_H\deq\mathbb E(hg),$$
	where the $\mathbb E$ in the definition of $H$ is the Bochner integral and $H\subseteq B$ by definition of the Bochner integral.
\end{defi}
\begin{rem}
	It can easily be seen that this definition coincides with the usual definition of a RKHS for Gaussian processes if the Banach space valued random variable is induced by some Gaussian process as in remark \ref{rem:GPandGrvs}.
\end{rem}

\begin{rem}[RKHS of Brownian Motion]\label{rem:RKHSofBM}
	Considering Brownian motion as a Banach space valued random variable as in remark \ref{rem:GPandGrvs}, it can be shown that the RKHS $H$ of the Wiener process on $T=[0,1]$ is $$H = \{h:[0,1]\rightarrow\mathbb R, h'\in L^2([0,1]), h(0)=0\},$$ with inner product induced by the usual Sobolev space norm $$\|h\|_H\deq \|h'\|_{L^2([0,1])}.$$
	Note that this is actually a norm because of the fixed boundary condition $h(0)=0$.
\end{rem}

Taking, for example, the trigonometric basis of $L^2([0,1])$, we get (by taking antiderivatives) an orthonormal basis $h_0(t)\deq t, h_k(t)\deq \frac{\sqrt 2}{\pi k}\sin(\pi kt)$ of $(H,(\cdot,\cdot)_H)$. Knowing that one can write Brownian motion as $$B(t)=\sum_{k=0}^\infty h_kg_k,$$ with $g_k\iidnormal$ one might expect this to hold in more general cases.
Indeed we have the following theorem:

\begin{thm}[Karhunen-Lo\`eve Expansion]
	Let $X$ be a centered (i.e. $\mathbb E f(X)=0 \forall f\in B^*$) Gaussian random variable in a separable Banach space $B$, $(h_k)_k$ an orthonormal basis of the RKHS $(H,(\cdot,\cdot)_H)$ of $X$, $g_k\iidnormal$, then
	$$X=\sum_k h_kg_k,$$
	almost surely with convergence of the sum in $B$.
\end{thm}

\begin{rem}[$X$ is in its RKHS iff dim $H<\infty$]\label{rem:XnotinRKHS}
	Note that $\mathbb E\|X\|_H^2 = \sum_{k=1}^{\text{dim }H}\mathbb E h_k^2$ by the previous theorem and absolute convergence of sum and $\mathbb E h_k^2 = 1$ for all $k$. Hence, by the $0-1$ law for Gaussian processes ($\mathbb E\|X\|_H^2=\infty \Rightarrow \exists\Omega_0\subseteq\Omega: \mathbb P(\Omega_0)>0$ on which $\|X(\omega)\|_H^2=\infty$), since $H$ is closed (hence measurable\footnote{Closed subsets $C$ of a topological space equipped with its Borel-$\sigma$-algebra $\mathcal B$ are $\mathcal B$-measurable.} in $B$, we see that $$\mathbb P(X\in H)=0$$
	if and only if $\text{dim }H=\infty$.
\end{rem}

\begin{rem}[In what space will $X$ be?]\label{rem:spaceofX}
	Now given the RKHS $H$ it is not immediate in what space $B$ the random variable $X$ (induced by the Karhunen-Lo\`eve expansion, i.e. $X\deq \sum_k h_kg_k$ using the notation from the theorem) will be. Indeed, this choice is rather arbitrary in general, as can be seen in the first section of \cite{She07} about abstract Wiener spaces.
	
	However, we may, informally, regard $B$ as a subset of a ``dual space'' $H'$, which is defined as the set of all $h=\sum_{k=1}^{\text{dim }H}h_ke_k$ (where $e_k$ is an \emph{ordered} orthonormal basis of $H$) with the property that the partial sums$\sum_{k=1}^n h_kf_k$ converge in $\mathbb R$ for every $f=\sum_{k=1}^{\text{dim }H}f_ke_k$.
	
	The reason to require a fixed ordering is the following calculation\todo{actually it always converges against the same limit, no matter which order we take, so shouldn't that take care of this restriction?}, where $h^n\deq \sum_{k=1}^n h_ke_k$ and $g_k\iidnormal$:
	
	\underline{$L^2$ convergence:}
	$$\mathbb E(h^n,f)_H^2 \eqbydef \mathbb E(\sum_{k=1}^n g_k f_k)^2 \eqby{g_k\iidnormal} \sum_{k=1}^n f_k^2$$ converges against $\|f\|_H^2$.
	
	\underline{Almost sure convergence:} $L^2$ convergence implies convergence in probability, which in turn, by Levy's equivalence theorem (the $g_k$ are independent), implies almost sure convergence of the \emph{partial sums} given a fixed ordering.
	
	%This restriction is not artificially introduced by a lack of means to show the stronger result of $X\eqbydef \sum_k h_kg_k$ being in the topological dual space $H^*$ since this would, by Riesz, imply that $X$ is actually in $H$ - a contradiction to remark \ref{rem:XnotinRKHS} in our infinite dimensional setting.
\end{rem}

While we \emph{could} define the Gaussian free field that way, i.e. as a sum of orthonormal basis elements weighted by iid normals, it has the aesthetic drawback of being somewhat arbitrary. Instead we will be slightly less explicit and define it to be the dual process of $X$ as described in remark \ref{rem:spaceofX} with the following adaption of the RKHS of the Wiener process: Noting that Brownian motion on $[0,1]$ is induced by the Sobolev space $H^1([0,1])$ it seems natural to investigate the random variable induced by the RKHS $H^1(D)$ for some domain $D\subseteq\mathbb R^d$. An easy (formal) calculation also gives the covariances for the dual process as given in the following definition:

\begin{defi}[Gaussian Free Field]
	The Gaussian free field on a domain $D\subseteq\mathbb R^d$ is the centered Gaussian process $$h\deq ((h,f)_\nabla : f\in H(D))$$ with covariance structure $$\mathbb E[(h,a)_\nabla(h,b)_\nabla]\deq (a,b)_\nabla.$$
\end{defi}
Note that this is well-defined by lemma \ref{lem:existenceofGPgivenCov}.
\begin{rem}
	It follows immediately that $f\mapsto (h,f)_\nabla$ is linear and that for every $f\in H(D)$ we have that $(h,f)_\nabla\sim\mathcal N(0,\|f\|_{H(D)})$.
\end{rem}

\begin{rem}
	Note that Dirichlet boundary conditions are ``hidden'' in the vanishing boundary conditions on elements in $H(D)$. %Furthermore we will assume that $h$ is zero outside of $D$ (which will take care of technical problems when averaging over circles near the boundary later on)
\end{rem}From now on we will restrict ourselves to the case $D\subseteq\mathbb R^2$. For what follows it will be useful to define the Green's function $G_D$ of a domain $D$ and stating some of its basic properties:

\begin{defi}[Green's function]
	For $x\in D$ fixed we let $$G_D^x(\cdot)\deq -\log|x-\cdot|-\tilde G_D^x(\cdot),$$ where $\tilde G_D^x$ is the unique harmonic function chosen such that $G_D^x(y)=0$ for all $y\in\partial D$.
	If it is clear from context what $D$ is we will drop it and simply write $G^x$ for $G^x_D$.
	To emphasise symmetry we will also sometimes write $G(x,y)$ for $G^x(y)$.
	
	We also define the corresponding integral operator $$[-\Delta^{-1}\rho](x)\deq\int_D G(x,y)\rho(y)dy,$$ which is the inverse of $-\Delta$.
\end{defi}

\begin{rem}[Importance of Green's Function]\label{rem:usesofGreensfct}
	One of the reasons the Green's function is so important is that it can be seen as the distributional solution of $-\Delta G^x = \delta_x$. 
	Noting that by integration by parts\todo{for smooth rho the derivative of a distribution is actually defined by something resembling the integration by parts formula - make that precise} we view $(h,\rho)\deq -(h,\Delta^{-1}\rho)_\nabla$ as being well-defined for all $\rho$ that can be written as $\rho = -\Delta f$ for some $f\in H(D)$.
\end{rem}

\begin{rem}[Qualitative Difference of GFF in Different Dimensions]
	One might wonder why a Wiener process, which can be seen as a Gaussian free field on $[0,1]$, is a H\"older continuous function while the Gaussian free field on any domain $D\subseteq \mathbb R^2$ is not even a function (i.e. does not have values at points), but merely a distribution.
	
	Firstly, we note that this is not as surprising as it may seem if we (equivalently!) chose to introduce Gaussian free fields as in remark \ref{rem:spaceofX} since H\"older continuous functions are not in the RKHS anymore as well.
	
	Furthermore remark \ref{rem:usesofGreensfct} suggests that the different behaviour of the Green's function in different dimensions is the reason\footnote{One might also present a different argument, as in \cite{She07}, arguing with the qualitatively different decays of eigenvalues of the Laplacian in different dimensions.} we cannot define the GFF pointwise for dimensions $d\geq 2$: Assume we could, then make the ansatz: $h(x)=(h,\delta_x)\eqbydef -(h,\Delta^{-1}\delta_x)_\nabla = -(h,G^x)_\nabla.$ Now it is easy to see\footnote{Note that we only defined the Green's function in dimension two, but it can be defined more generally in arbitrary dimensions as the distributional solution of $-\Delta G^x=\delta_x$ so that the above ansatz makes sense.} that while $G^x\in H([a,b])$, it fails to be in $H(D)$ in dimensions $d\geq 2$ (i.e. for $D\subseteq\mathbb R^2$).
\end{rem}

We recall the following properties of Green's functions in $\mathbb C$:

\begin{pro}[Properties of $G_D^x$]\label{prop:propertiesofGreensfcts} For $D\subseteq\mathbb C$ fixed and $\tilde G^x$ as in the definition we have
	\begin{enumerate}
		\item $G(x,y) = G(y,x)$ for all $x,y\in D\setminus\{x\}$. (symmetry)
		\item $\Delta G_D^x(y)=0$ for all $y\in D\setminus\{x\}$, i.e. it is harmonic in $D\setminus\{x\}$. (harmonicity)
		\item\label{item:GreensfctatXisconfRadius} $\tilde G^x(x)=\log C(x;D)$ for all $x\in D$.
	\end{enumerate}
\end{pro}

\subsection{Properties}

\subsubsection{Covariances}

%one might start directly with the more formal definition of a GFF as a dual process of some rv $h$ with the covariances induced by the following hilbert space isomorphism:
\todo{discuss this alternative approach with Nickl}

%\begin{lem}[Covariances]
%$$\mathbb E[(h,a)_\nabla (h,b)_\nabla] = (a,b)_\nabla$$
%\end{lem}

integrating by parts and writing $[-\Delta^{-1}\rho](x) = \int_D G^x(y)\rho(y)dy$ directly gives the following corollary

\begin{cor}
	$$\mathbb E[(h,\rho_1) (h,\rho_2)]=\int_{D\times D}\rho_1(x)G(x,y)\rho_2(y)dxdy,$$
for all $\rho_1,\rho_2 \in (-\Delta)H(D)$
\end{cor}

\subsubsection{Markov Property}

\begin{rem}[Motivation by Brownian Motion]
	markov property of BM can be interpreted (equivalently stated as) BLABLABLA
\end{rem}

\begin{defi}
	$H_U, H_U^\perp$
\end{defi}

\begin{thm}\label{thm:MarkovProperty}
	$H_U, H_U^\perp$ span $H(D)$
\end{thm}

so that is what we call Markov property in this case.

\todo{do I need to elaborate on the field exploration and general closed U part of Sheffield07?}

\subsubsection{Circle Averages}

Although $h$ is not a function (i.e. it does not have values at points), we shall see circle averages are well-defined by duality. To see this first introduce the $\eps$-regularized Green's function:

\begin{defi}
	We let
	$$G_{D,\eps}^x(\cdot)\deq -\log[|x-\cdot|\vee \eps]-\tilde G_{D,\eps}^x,$$
	where, as before, $\tilde G_{D,\eps}^x$ is the unique harmonic (in $D$) function such that $G_{D,\eps}^x(y)=0$ for all $y\in \partial D$. In case it is clear from context what $D$ is we will also drop it and simply write $G_\eps^x$ for $G_{D,\eps}^x$.
\end{defi}

Now this is harmonic in $B_\eps(x)$ (since it is and does not have a singularity anymore) and in $\overline{B_\eps(x)}^c$ (as a sum of two harmonic functions) so its Laplacian will be supported on $\partial B_\eps(x)$. So one might expect that $-\Delta G_\eps^x$ is some kind of uniform measure on $\partial B_\eps(x)$. 

Indeed, using the rotational symmetry and lack of (classical) differentiability at $\partial B_\eps(x)$ of the derivative, the following is easy to see:

\begin{pro}\label{prop:circleAvg}
	for any $x\in D, \eps>0$ with $\eps$ small enough\footnote{$\eps$ such that $\eps<d(x,\partial D)$} we have that 
	\begin{enumerate}
		\item $G_\eps^x\in H^1(D)$
		\item $-\Delta[G_\eps^x] = \nu_\eps^x$, where $\nu_\eps^x$ is the uniform measure of total mass one on $\partial B_\eps(x)$.
	\end{enumerate}
\end{pro}

While (1) ensures that $(h,G_\eps^x)_\nabla$ is well defined, (2) states that $(h,G_\eps^x)_\nabla = (h,\nu_\eps^x)$ can be interpreted as a circle average of $h$ with radius $\eps$ around $x$.
\begin{proof}
	Write it in polar coordinates and use that $\Delta = \frac{\partial^2}{\partial r^2}+\frac{1}{r}\frac{\partial}{\partial r}+\frac{1}{r^2}\frac{\partial^2}{\partial\theta^2}$, where the last term vanishes by rotational symmetry.
\end{proof}

Thus the following definition is well-posed and can be interpreted as circle average:

\begin{defi}[$\eps$-regularized GFF $h_\eps$]
	$$h_\eps(z)\deq (h,G_\eps^z)_\nabla$$
\end{defi}

Since working directly with $h$ is not possible in what is to come we will want to approximate $h$ with $h_\eps$ by letting $\eps$ tend to zero. In those calculations it will turn out to be helpful to see that for any fixed $z\in D$ and suitably chosen $t_0$ the process $(h_{e^-t}(z))_{t\geq t_0}$ is a standard Brownian motion.

\begin{lem}[Circle Averages Are Brownian Motion as Radius Decreases]\label{lem:hepsIsBM}
	Let $h$ be a Gaussian free field in $D$, $z\in D$ fixed, $t_0\deq\inf\{t\geq 0: B_{e^{-t}}\subseteq D\}$, and $$\mathcal B_t\deq h_{e^{-(t+t_0)}}(z)-h_{e^{-t_0}}(z).$$
	Then $(\mathcal B_t)_{t\geq 0}$ is a standard Brownian motion.
\end{lem}

Before we start with the proof of the lemma we first express the covariances of the $\eps$-regularized Gaussian free field $h_\eps$ in terms of the conformal radius $C(z;D)$ of $D$ viewed from $z\in D$ which is defined as $C(z;D)\deq \phi'(z)^{-1}$, where $\phi:D\rightarrow\mathbb D$ is the unique\footnote{This is well-defined by the Riemannian mapping theorem.} conformal map from $D$ to $\mathbb D$ mapping $z$ to $0$ such that $\phi'(z)\geq 0$.

\begin{pro}[Covariance Structure of $h_\eps$]\label{prop:covariancesOfRegularizedH}
	Writing $$G_{\eps_1,\eps_2}(z_1,z_2) \deq \mathbb E[h_{\eps_1}(z_1)h_{\eps_2}(z_2)] = \int_{D\times D} G^x(y)d\nu_{\eps_1}^{z_1}(x)d\nu_{\eps_2}^{z_1}(y),$$ for $\nu_\eps^z$ the uniform measure of total mass one on $\partial B_\eps(z)$ as before, we have
	\begin{enumerate}
		\item $G_{\eps_1,\eps_2}(z_1,z_2) = (G_{\eps_1}^{z_1},G_{\eps_2}^{z_2})_\nabla = (G_{\eps_1}^{z_1},\nu_{\eps_2}^{z_2})$, the mean value of $G_{\eps_1}^{z_1}$ on $\partial B_{e_2}(z_2)$.
		\item If $B_{\eps_1}(z_1)$ and $B_{\eps_2}(z_2)$ are disjoint and in $D$, then $$G_{\eps_1,\eps_2}(z_1,z_2) = G(z_1,z_2),$$ the usual (i.e. not regularized) Green's function.
		\item\label{item:covarianceForSameZdifferentEps} If $B_{\eps_1}(z)\subseteq D$ and $\eps_1\geq\eps_2$ then $$G_{\eps_1,\eps_2}(z,z)=-\log\eps_1+\log C(z;D).$$
	\end{enumerate}
\end{pro}

\begin{proof}[Proof of proposition]
	(1) follows directly by definition and partial integration, using proposition \ref{prop:circleAvg}.
	
	(2) follows by the circle average property of harmonic functions, using that $G^x$ is harmonic in $D\setminus\{x\}$ and coincides with $\tilde G^x_\eps$ in $D\setminus B_\eps(x)$.
	
	(3) holds by the following calculation:
	\begin{align*}
		G_{\eps_1,\eps_2}(z,z)&\eqby{(1)}(G_{\eps_1}^{z_1},\nu_{\eps_2}^{z_2})\\
		&\eqbydef \int_\mathbb{C}(-\log(\eps_1\vee |z-y|)-\tilde G_{\eps_1}^z(y))d\nu_{\eps_2}^z(y),
	\end{align*}
	where the first summand equals $-\log\eps_1$ since, by definition of $\nu_{\eps_2}^z$, we integrate only over $y\in\partial B_{\eps_2}(z)$ and $\int_\mathbb{C}d\nu_{\eps_2}^z(y)=1$, and the second summand evaluates to $\tilde G^z(z)$ by the circle average property of harmonic functions and because $\eps_1\leq\text{dist}(z,\partial D)$ (so that the harmonic correction term does not ``see'' the change around $z$). Using proposition \ref{prop:propertiesofGreensfcts}(\ref{item:GreensfctatXisconfRadius}) this is exactly what we wanted to show.
\end{proof}

\begin{proof}[Proof of lemma]
	Clearly $(\mathcal B_t)_{t\geq 0}$ is a centered Gaussian process. Hence it suffices to show that for any $0\leq s\leq t$ we have that $\mathbb E[\mathcal B_s\mathcal B_t] = s$. 
	
	By definition we have $$\mathbb E[\mathcal B_s\mathcal B_t]\eqbydef \mathbb E[(h_{e^{-(s+t_0)}}(z)-h_{e^{-t_0}}(z))(h_{e^{-(t+t_0)}}(z)-h_{e^{-t_0}}(z))],$$ so using linearity of expectation and proposition \ref{prop:covariancesOfRegularizedH}(\ref{item:covarianceForSameZdifferentEps}) repeatedly one easily arrives at $\mathbb E[\mathcal B_s\mathcal B_t] = s$.
\end{proof}

Note that one could have done the same calculations with the $\eps$ parameterization to get some logarithmic terms in the end (instead of the correct $s$) - this way it would become obvious why we chose the $e^{-t}$ parameterization.

\section{Liouville Measures}

In this section we want to study two dimensional random surfaces. Similarly to how Brownian motion (the one dimensional Gaussian free field) is a ``random line'', a Gaussian free field in some bounded domain $D\subseteq\mathbb C$ can be identified with a random Riemann surface via the correspondence suggested by the following theorem:
\begin{thm}[Riemann uniformization theorem]
	For every smooth, simply connected Riemannian manifold\footnote{Note that a surface can be described by several quantities like the area of any fixed measurable subset, its geodesics, the length of any fixed, smooth curve, etc. Here we will focus on the area.} $(\mathcal M,\mu)$ there exists a real valued function $\lambda:D\rightarrow\mathbb R$ for $D\in\{\mathbb C,\mathbb C\cup\{\infty\},\mathbb D\}$ such that the Radon Nikodym derivative of $\mu$ at $z\in D$ is given by $e^{\lambda(z)}$.
\end{thm}

\subsection{Existence of Weak Limits}

Now we would like to take $\lambda = \gamma h$ (for some fixed parameter $\gamma\geq 0$) to get two dimensional random surfaces, but since $h$ does not have values at points this is not well-defined. Instead, we will take $\lambda = \gamma h_\eps$ and see ``in which order of magnitude this approximation typically explodes'' as $\eps$ goes to zero - or to be more precise: Find $c,\alpha$ such that $\mathbb E e^{\gamma h_\eps}(z) \rightarrow c\eps^\alpha$.

Normalising by this factor, i.e. taking $\lambda=\gamma h_\eps - \alpha\log\eps$, so that $\mathbb E e^{\gamma h_\eps(z) - \alpha\log\eps}=O(1)$, will be our best shot at finding a well defined limiting measure.

\begin{rem}[Asymptotics of $\mathbb E e^{\gamma h_\eps(z)}$]
	Noting that $\mathbb E e^{\mathcal N(a,b)}=e^{a+b/2}$ and $h_\eps(z)\sim\mathcal N(0,-\log\eps+\log C(z;D))$ by \ref{prop:covariancesOfRegularizedH}(\ref{item:covarianceForSameZdifferentEps}) we get that $$\mathbb E e^{\gamma h_\eps(z)} = e^{\gamma^2/2(-\log\eps+\log C(z;D))}=\left(\frac{C(z;D)}{\eps}\right)^{\gamma^2/2} = c\eps^{-\gamma^2/2}.$$
	So it seems natural to normalise by a factor of $\eps^{\gamma^2/2}$:
\end{rem}

\begin{defi}[Regularized $h_\eps$]
	Write $\overline h_\eps\deq \gamma h_\eps+\frac{\gamma^2}{2}\log\eps$.
\end{defi}


Intuitively a weak limit should be strong enough to preserve information about area since (at least for probability measures) we have $\mu_n\rightarrow\mu$ weakly implies that $\mu_n(B)\rightarrow\mu(B)$ for all $B$ in the Borel-$\sigma$ algebra with $\mu(\partial B)=0$.

\begin{thm}[Weak Limit of $\eps^{\gamma^2/2}e^{\gamma h_\eps}$]
	Let $h$ be a Gaussian free field in some bounded domain $D\subseteq\mathbb C$, $\gamma \in [0,2)$ fixed, and $\mu_\eps\deq\eps^{\gamma^2/2}e^{\gamma h_\eps(z)}dz = e^{\overline h_\eps(z)}dz$. Then $\mu_\eps$ converges weakly almost surely against some $\mu=\mu_h$ for $\eps=2^{-k}\rightarrow 0$ as $k\rightarrow \infty$.
\end{thm}
\begin{proof}
	\underline{Reduction to squares:} It suffices to show that $\mu_{2^{-k}}(S)$ converges almost surely against some finite limit $\mu(S)$ for every diadic square compactly supported in $D$.
	
	To see this note that there are only countably many such squares $\{S_i\}_{i\in\mathbb N}$, so if we show that $\mu_{2^{-k}}(S_i)\rightarrow\mu(S_i)$ on $\Omega_i$ with $\mathbb P(\Omega_i)=1$, then this still holds almost surely for all squares at once. Now we want to show that $\mu_{2^{-k}}(f)\rightarrow\mu(f)$ on some $\Omega_0$ with $\mathbb P(\Omega_0)=1$ for all $f\in C_b(D)$. To see that, note that $\mathcal S\deq\{\sum_{i=1}^N a_i 1_{S_i}:N\in\mathbb N, a_i\in\mathbb R, S_i \text{ disjoint, diadic squares}\}$ are dense in $L^1(\mu_{2^{-k}})$ (hence in particular in $C_b(D)$ as $\mu_{2^{-k}}$ is a Radon Nikodym derivative of the Lebesgue measure, so continuous functions are measurable) just as simple functions. So for $f_N = \sum_{i=1}^N a_i^N1_{S_i^N}\in\mathcal S$ such that $f_N\rightarrow f$ in $L^1(\mu_{2^{-k}})$ almost surely\footnote{Note that it is important to look at the \emph{countable} family of diadic squares.} we can write
	\begin{align*}
		\mu_{2^{-k}}(f)&=\mu_{2^{-k}}\left(\lim_N\sum_{i=1}^N a_i^N1_{S_i^N}\right)\\
		&= \lim_N\sum_{i=1}^N a_i^N\mu_{2^{-k}}(S_i^N) =\lim_N \mu_{2^{-k}}(f_N)
	\end{align*}
	Taking $k\rightarrow\infty$ we only need to show that $$\lim_k\lim_N \mu_{2^{-k}}(f_N) = \lim_N\lim_k \mu_{2^{-k}}(f_N),$$
	to get that $\mu_{2^{-k}}(f)\rightarrow \mu(f)$ almost surely as $k\rightarrow\infty$.
	This is justified by the Moore-Osgood theorem if
	\begin{itemize}
		\item $\lim_N \mu_{2^{-k}}(f_N)=\mu_{2^{-k}}(f)$ uniformly for all $k<\infty$.\footnote{This notation should emphasize that $k=\infty$ need not be checked, even though it is part of the extended natural numbers $\mathbb N\cup\{\infty\}$ we are working with here. Also we write $f$ instead of $f_\infty$ and $\mu$ instead of $\mu_{2^{-\infty}}$.}
		\item $\lim_k \mu_{2^{-k}}(f_N)=\mu(f_N)$ for all $N$ fixed.
	\end{itemize}
	The first requirement follows almost surely since for all $c>0$ we have by almost sure $L^1$ convergence $$\mathbb P\left(\sup_{k<\infty}|\mu_{2^{-k}}(f_N)-\mu_{2^{-k}}(f)|\geq c \;\;\forall N\in\mathbb N\right)=0.$$
	The second follows by definition of infinite sums.
	Note that we could not use the Portmanteau theorem since we normalised the measures so that they are $O(1)$ as $\eps\rightarrow 0$; so while they are finite with probability one (since their expectation is), they need not be probability measures.

	
	\underline{Reduction to exponential decay:} To show that $\mu_{2^{-k}}(S)\rightarrow\mu(S)<\infty$ with probability one, we show that $\mathbb E|\mu_{2^{-(k+1)}}(S)-\mu_{2^{-k}}(S)|$ decays exponentially in $k$.
	
	Writing $X_k\deq \mu_{2^{-k}}(S)$, a real valued random variable, the exponential decay implies that $(X_k)_k$ is a Cauchy sequence in $L^1(\mathbb P)$, but the $L^1$ is complete and hence the limit $X=\mu(S)$ is in $L^1$ again. Since its expectation is finite it implies that $\mu(S)<\infty$ with probability one.
	
	\underline{Strategy to prove exponential decay:} \todo{choose better title for this subsection}\todo{where does it help to take the second moment instead of showing it directly for the first?}To show that $\mathbb E|\mu_{2^{-(k+1)}}(S)-\mu_{2^{-k}}(S)|$ decays exponentially in $k$ we assume without loss of generality\footnote{Otherwise we would have to keep writing an additional factor given by the (Lebesgue) area of $S$.} that $S=[0,1]^2$, so that $\mu_\eps(S)$ is just the mean value of $e^{\overline h_\eps}$ on $S$ and calculate $\mathbb E(|\mu_{2^{-(k+1)}}(S)-\mu_{2^{-k}}(S)|^2)$ (note that showing exponential decay of the second moment is enough since the first moment $\mathbb Ee^{\overline h_\eps(z)}=C(z;D)^{\gamma^2/2}$ is positive and bounded from below and above uniformly in $z\in S$ since we assumed that $S$ is compact in $D$ open and the conformal radius can be bounded from below and above by $\text{dist}(z,\partial D)$):
	
	We would like to formalize the intuition that, by the Markov property, $h_\eps(z)$ and $h_\eps(z')$, conditioned on $h_{2\eps}(z)$ and $h_{2\eps}(z')$ respectively, are independent (for $\eps$ small enough) which should make calculations easier. In the following we will take the average values of $\overline h_{2^{-(k+1)}}$ and $\overline h_{2^{-(k+2)}}$, conditioned on $\overline h_{2^{-(k+1)}}$, over some lattice chosen $S_k^y$ such that all those values are independent. Then we will average over all those lattices $(S_k^y)_{y\in S}$ and take another expectation to get rid of the conditioning.
	
	To make that rigorous we define \begin{itemize}
		\item $S_k^y\deq (y+2^{-k}\mathbb Z)\cap D$
		\item $A_k^y\deq \frac{1}{|S_k^y|}\sum_{z\in S_k^y}exp(\overline h_{2^{-(k+1)}}(z))$
		\item $B_k^y\deq \frac{1}{|S_k^y|}\sum_{z\in S_k^y}exp(\overline h_{2^{-(k+2)}}(z))$
	\end{itemize}
	the average values of $h_{2^{-(k+1)}}$ and $h_{2^{-(k+2)}}$ over $S_k^y$, respectively.
	
	\underline{Reduction to disjoint balls:} Writing $\mathbb E^y$ for the expectation over $y\in S$ (given by the Lebesgue measure) we have \begin{align*}
		\mathbb E|\mu_{2^{-k}}(S)-\mu_{2^{-(k+1)}}(S)| &= \mathbb E|\mathbb E^y(exp(\overline h_{2^{-k}})-exp(\overline h_{2^{-(k+1)}}))|\\
		&=\mathbb E|\mathbb E^y(A_k^y-B_k^y)\\
		&\leqby{\text{Jensen}}\mathbb E\mathbb E^y|A_k^y-B_k^y|\\
		&\eqby{\text{Tonelli}}\mathbb E^y\mathbb E|A_k^y-B_k^y|
	\end{align*}
	
	so it suffices to show that $\mathbb E|A_k^y-B_k^y|$ decays exponentially in $k$, uniformly for all $y\in S$. (Note that we can apply Jensen since $\mathbb E^y(1)=\int_S ds=1$ since we assumed that $S=[0,1]^2$. Otherwise we would have had to introduce a normalising factor.)
	
	\underline{Getting a conditional expectation:} Now by the Markov property (theorem \ref{thm:MarkovProperty}) of the Gaussian free field we have that the random variables $h_{2^{-(k+2)}}(z)$ conditioned on $h_{2^{-(k+1)}}(z)$ are independent of one another for different $z\in S_k^y$. Moreover, by lemma \ref{lem:hepsIsBM}, we have that they are $\mathcal N(h_{2^{-(k+1)}}(z),\log 2)$. Using these results an elementary (but tedious) calculation shows that
	\begin{align*}
		\mathbb E[|A_k^y-B_k^y|^2|\{h_{2^{-(k+1)}}(z)\}_{z\in S_k^y}] &\eqbydef 2^{-4k}\sum_{z\in S_k^y}\mathbb E[|e^{\overline h_{2^{-(k+1)}}(z)}-e^{\overline h_{2^{-(k+2)}}(z)}|^2|\{h_{2^{-(k+1)}}(z)\}_{z\in S_k^y}]\\
		&=2^{-4k}C\sum_{z\in S_k^y}\left(e^{\overline h_{2^{-(k+1)}}(z)}\right)^2,\numberthis \label{eq:conditionalExpOfAkBksquared}
	\end{align*}
	where $C>0$ is some constant only depending on $\gamma$.
	
	\underline{Showing exponential decay for the unconditional expectation:} Now we can take expectations to get the unconditional expectation $\mathbb E|A_k^y-B_k^y|^2$, but it turns out that the naive approach of bounding the expectation by the second moment only works for $\gamma<\sqrt 2$ since
	
	\begin{align*}
		\mathbb E[|A_k^y-B_k^y|^2] &\eqby{\eqref{eq:conditionalExpOfAkBksquared}}
		2^{-4k}C\sum_{z\in S_k^y}\mathbb E\left[(e^{\overline h_{2^{-(k+1)}}(z)})^2\right]\\
		&= O\left(|S_k^y|2^{-4k}2^{-k\gamma^2}\mathbb E[e^{2\gamma h_{2^{-(k+1)}}}]\right) \\
		&= O(2^{2k}2^{-4k}2^{-k\gamma^2}2^{2k\gamma^2})\\
		&= O(2^{-k(2-\gamma^2)}),
	\end{align*}
	which finishes the proof for $\gamma<\sqrt 2$ (because we have exponential decay in that regime).
	
	The more general result for $2\leq\gamma^2<4$ can be obtained by showing that the second moment is only made large by rare occurrences of $h_\eps(z)$ being much larger than expected and that their contribution to the first moment (the expectation) vanishes exponentially.
\end{proof}

\subsection{Parameterizing a Surface with Different Domains}
for some calculations we would like to parameterize the same surface with a different domain $\tilde D$. we do that using the following formula:\todo{write proper motivation, should involve the connection to SLEs somehow}

\begin{pro}[Change of Coordinates]
	Let $h$ be a Gaussian free field on a domain $D$, $\psi$ a conformal map from a domain $\tilde D$ to $D$, and $\tilde h\deq h\circ\psi+Q\log|\psi'|$ a distribution on $\tilde D$, where $Q\deq \frac{2}{\gamma}+\frac{\gamma}{2}$.
	
	Then $$\mu_{\tilde h}=\mu_h\circ\psi,$$
	i.e. $\mu_{\tilde h}(A)=\mu_h(\psi(A))$ for all Borel measurable $A\subseteq \tilde D$.
\end{pro}
\begin{rem}
	First we recall that $(\cdot,\cdot)_\nabla$ is conformally invariant in two dimensions and that, by the Cauchy-Riemann equations, we have that the determinant of a Jacobian for a conformal coordinate change $\psi$ is given by $|\psi'|^2$.
\end{rem}
\begin{proof}
	If $(f_i)_{i\in\mathbb N}$ is an orthonormal basis of $H(D)$ as in the statement of theorem \ref{thm:approximationofhwithhn}, then, by the conformal invariance of $(\cdot,\cdot)_\nabla$, we have that $(f_i\circ\psi)_{i\in\mathbb N}$ is an orthonormal basis of $H(\tilde D)$ and, by theorem \ref{thm:approximationofhwithhn}, $h^n\circ \psi \eqbydef h^0\circ\psi+\sum^n_{i=1}(h,f_i)_\nabla f_i\circ\psi = h^0\circ\psi+\sum^n_{i=1}(h,f_i\circ\psi)_\nabla f_i\circ\psi$ converges weakly to the Gaussian free field on $\tilde D$. 
\end{proof}

to prove it we assumed the following theorem which is also of independent interest

\begin{thm}\label{thm:approximationofhwithhn}
	Write $h\deq \overline h+h^0$ where $\overline h$ is the zero boundary Gaussian free field on some domain $D$ and $h^0$ is a deterministic continuous function on $D$. Let $(f_i)_{i\in\mathbb N}$ be an orthonormal basis of continuous functions for $H(D)$ and let $h^n\deq h^0+\sum_{i=1}^n (h,f_i)_\nabla f_i$. Then $\mu=\mu_h$ is almost surely the weak limit of $$\mu^n\deq exp\left(\gamma h^n(z)-\frac{\gamma^2}{2}\text{Var}h^n(z)+\frac{\gamma^2}{2}\log C(z;D) \right)dz$$
	as $n\rightarrow\infty$.
	
	Furthermore, for each measurable $A\subseteq D$ and for each $n\geq 0$ we have $$\mathbb E[\mu(A)|h^n]=\mu^n(A).$$
\end{thm}

before we get to the proof we define rooted random measures and record some of their properties which will be used in the ensuing proof

\begin{defi}[Rooted Random Measures]
	
\end{defi}
\begin{rem}[$\Theta_\eps$ is well-defined]
	as radon nikodym derivative of dzdh
\end{rem}

\begin{rem}[Marginal Laws and Conditional Distributions]
	
\end{rem}

\begin{proof}[Proof of theorem above]
	
\end{proof}

\section{Connection to SLEs}
another way to sample a GFF is by first sampling a BM (which determines an SLE) and then setting $\tilde h\deq ZEUG(f_T+arg+...)$.

\begin{rem}[Geometric Intuition]
	
\end{rem}

see that a GFF $h$ (as a (dual) GP) is characterised by its finite dimensional distributions since $(\eps,z)\mapsto h_\eps(z)$ is H\"older of BLA:

\begin{thm}[Continuity of $h_\eps(z)$]
	
\end{thm}
\begin{proof}
	
\end{proof}

we will now show the theorem by showing that the finite dimensional distributions coincide

\begin{thm}
	
\end{thm}
\begin{proof}
	
\end{proof}

\begin{thebibliography}{00}
\bibitem{She07} S. Sheffield: \emph{Gaussian free fields for mathematicians}, Arxiv: 0312099
%\bibitem{HK14} L. Holst, T. Konstantopoulos: \emph{Runs in coin tossing: a general approach for deriving distributions for functionals}, Arxiv: 1407.6831
%\bibitem{AGZ10} G. Anderson, A. Guionnet, O. Zeitouni: \emph{An Introduction to Random Matrices}, Cambridge University Press, 2010
\end{thebibliography}


\end{document}