\documentclass[11pt,reqno]{amsart}
%\allowdisplaybreaks[4]
\RequirePackage[OT1]{fontenc}
\RequirePackage{amsthm,amsmath}
\RequirePackage[numbers]{natbib}
\RequirePackage[colorlinks,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{amssymb}
\usepackage{anysize}
\usepackage{hyperref}
\usepackage{extarrows}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{stmaryrd}
\usepackage{color}
\usepackage{soul}
\usepackage{todonotes}
\usepackage[perpage]{footmisc}

%\usepackage{fontspec}
%\usepackage{arydshln}

\numberwithin{equation}{section}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defi}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{assu}[thm]{Assumption}
\newtheorem{Exa}{Example}[section]
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray*}}
\newcommand{\eea}{\end{eqnarray*}}
\renewcommand*{\bibfont}{\footnotesize}


\newcommand{\eqby}[1]{\mathrel{\stackrel{#1}{=}}}
\newcommand{\eqbydef}{\mathrel{\stackrel{\text{(def)}}{=}}}
\newcommand{\leqby}[1]{\mathrel{\stackrel{#1}{\leq}}}
\newcommand{\geqby}[1]{\mathrel{\stackrel{#1}{\geq}}}
\newcommand{\deq}{\mathrel{\mathop:}=}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % use as in http://tex.stackexchange.com/questions/42726/align-but-show-one-equation-number-at-the-end
%\begin{align*}
%a &=b \\
%  &=c \numberthis \label{eqn}
%\end{align*}

\newcommand{\iid}[1]{\mathrel{\stackrel{\text{iid}}{\sim}}#1}
\newcommand{\iidnormal}{\mathrel{\stackrel{\text{iid}}{\sim}}\mathcal N(0,1)}

\newcommand{\mR}{\mathbb R^n}
\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\makeatletter % `@' now normal "letter"
\@addtoreset{equation}{section}
\makeatother  % `@' is restored as "non-letter"
\renewcommand\theequation{{\thesection}%
                   .{\arabic{equation}}}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\abs}[1]{\lvert#1\rvert}

%    Blank box placeholder for figures (to avoid requiring any
%    particular graphics capabilities for printing this document).
\newcommand{\blankbox}[2]{%
  \parbox{\columnwidth}{\centering
%    Set fboxsep to 0 so that the actual size of the box will match the
%    given measurements more closely.
    \setlength{\fboxsep}{0pt}%
    \fbox{\raisebox{0pt}[#2]{\hspace{#1}}}%
  }%
}

\renewcommand{\baselinestretch}{1.2}





\newcommand{\fa}{{\frak a}}
\newcommand{\bk}{{\bf{k}}}
\newcommand{\ttau}{\vartheta}

\newcommand{\LL}{L}
\newcommand{\fn}{{\mathfrak n}}

\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\const}{\mbox{const}}
\newcommand{\La}{x}
\newcommand{\beqa}{\begin{eqnarray}}
\newcommand{\eeqa}{\end{eqnarray}}
%\newcommand{\e}{\varepsilon}
\newcommand{\eps}{\varepsilon}
\newcommand{\pt}{\partial}
\newcommand{\rd}{{\rm d}}
\newcommand{\bR}{{\mathbb R}}
\newcommand{\bC}{{\mathbb C}}

\newcommand{\bZ}{{\mathbb Z}}
\newcommand{\non}{\nonumber}
\newcommand{\wH}{{K}}


\newcommand{\bke}[1]{\left( #1 \right)}
\newcommand{\bkt}[1]{\left[ #1 \right]}
\newcommand{\bket}[1]{\left\{ #1 \right\}}
\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\Norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\bka}[1]{\left\langle #1 \right\rangle}
\newcommand{\vect}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\K}{n}
\newcommand{\z}{{\underline z}}
\renewcommand{\u}{{\underline u}}

\newcommand{\tr}{\mbox{tr\,}}



\newcommand{\unu}{{\underline {\nu}}}
\newcommand{\umu}{{\underline {\mu}}}


\renewcommand{\Re}{\mathsf{Re}\,}
\renewcommand{\Im}{\mathsf{Im}\,}

\newcommand{\ba}{{\bf{a}}}
\newcommand{\bb}{{\bf{b}}}
\newcommand{\bx}{{\bf{x}}}
\newcommand{\by}{{\bf{y}}}
\newcommand{\bu}{{\bf{u}}}
\newcommand{\bv}{{\bf{v}}}
\newcommand{\bw}{{\bf{w}}}
\newcommand{\bz}{{\bf {z}}}
\newcommand{\bq}{{\bf {q}}}
\newcommand{\tbx}{\widetilde\bx}
\newcommand{\bh}{{\bf{h}}}
\newcommand{\bn}{{\bf{n}}}

\newcommand{\fq}{{\frak q}}
\newcommand{\fu}{{\frak u}}
\newcommand{\fh}{{\frak h}}
\newcommand{\bm}{{\bf  m }}
\newcommand{\bmu}{\mbox{\boldmath $\mu$}}
\newcommand{\sa}{{[\alpha ]}}


\newcommand{\tby}{\widetilde\by}

\newcommand{\bT}{{\T}}
\newcommand{\bO}{{\bf O}}

\newcommand{\bla}{\mbox{\boldmath $\lambda$}}
\newcommand{\bnu}{\mbox{\boldmath $\nu$}}
\newcommand{\bbeta}{\mbox{\boldmath $\beta$}}


\newcommand{\wG}{{\widehat G}}
\newcommand{\mg}{{m_H}}
\newcommand{\mW}{{m_W}}

\newcommand{\al}{\alpha}
\newcommand{\de}{\delta}




\newcommand{\barv}{ {[ v ]}}
\newcommand{\barZ}{ {[ Z ]}}

\newcommand{\ga}{{\gamma}}
\newcommand{\Ga}{{\Gamma}}
\newcommand{\la}{\lambda}
\newcommand{\Om}{{\Omega}}
\newcommand{\om}{{\omega}}
\newcommand{\si}{\sigma}
\renewcommand{\th}{\theta}
\newcommand{\td}{\widetilde}
\newcommand{\ze}{\zeta}

\newcommand{\cL}{{\mathscr L}}
\newcommand{\cE}{{\mathcal E}}
\newcommand{\cG}{{\mathcal G}}
\newcommand{\cX}{{\mathcal X}}
\newcommand{\cY}{{\mathcal Y}}
\newcommand{\cK}{{\mathcal K}}




\newcommand{\cM}{{\mathcal M}}
\newcommand{\cN}{{\mathcal N}}
\newcommand{\cH}{{\mathcal H}}
\newcommand{\cB}{{\mathcal B}}
\newcommand{\cU}{{\mathcal U}}

\newcommand{\bW}{{\bf W}}

\newcommand{\ov}{\overline}



%\newcommand{\re}{{\text Re \, }}
\newcommand{\re}{{\mathfrak{Re} \, }}
\newcommand{\im}{{\mathfrak{Im} \, }}
%\newcommand{\im}{{\text Im \, }}
\newcommand{\E}{{\mathbb E }}
\newcommand{\R}{{\mathbb R }}
\newcommand{\N}{{\mathbb N}}


\newcommand{\CC}{{\mathbb C }}
\newcommand{\RR}{{\mathbb R }}
\newcommand{\NN}{{\mathbb N}}
\newcommand{\g}{\gamma}
\newcommand{\ii}{\mathrm{i}}



\newcommand{\bl}{{\boldsymbol \lambda}}


\newcommand{\bt}{{\boldsymbol \theta}}
\newcommand{\htau}{{\hat\tau}}


\newcommand{\Ci}{{ C_{inf}}}
\newcommand{\Cs}{{ C_{sup}}}
\newcommand{\Z}{{\mathbb Z}}
\renewcommand{\P}{{\mathbb P}}


\newcommand{\C}{{\mathbb C}}
\newcommand{\pd}{{\partial}}

\newcommand{\nb}{{\nabla}}
\newcommand{\lec}{\lesssim}
\newcommand{\ind}{{\,\mathrm{d}}}

\renewcommand{\S}{\mathbb S}
\newcommand{\T}{\mathbb T}
\newcommand{\U}{\mathbb U}
\newcommand{\V}{\mathbb V}
\newcommand{\bU}{ {\bf  U}}
\newcommand{\bS}{ {\bf  S}}
\renewcommand{\S}{[\bf S]}

\newcommand{\ph}{{\varphi}}

\renewcommand{\div}{\mathop{\mathrm{div}}}
\newcommand{\curl}{\mathop{\mathrm{curl}}}
\newcommand{\spt}{\mathop{\mathrm{spt}}}
\newcommand{\wkto}{\rightharpoonup}
\newenvironment{pf}{{\bf Proof.}} {\hfill\qed}

\newcommand{\lv}{{\bar v}}
\newcommand{\lp}{{\bar p}}

\newcommand{\Cr}{\color{red}}
\newcommand{\Cb}{\color{blue}}
\newcommand{\Cg}{\color{green}}
\newcommand{\nc}{\normalcolor}

\newcommand{\e}{\varepsilon}


\marginsize{35mm}{35mm}{38mm}{40mm}

\begin{document}
\tableofcontents

\section*{Notation}
\begin{itemize}
	\item Abbreviate the Gaussian Free Field by GFF.
	\item For $X$ being a random variable on some probability space $(\Omega, \mathcal A, \mathbb P)$ write $\mathcal L(X)$ for the law of a random variable $X$, i.e. the pushforward measure $\mathcal L(X)\deq \mathbb P\circ X^{-1}$.
	\item For a topological space $S$ denote its Borel-$\sigma$-algebra by $\mathcal B_S$.
	\item For a topological vector space $V$ write $V^*$ for its topological/continuous dual space, i.e. the set of all continuous, linear functionals $f:V\rightarrow\mathbb R$.
	\item Unless stated otherwise the probability spaces we are working with will implicitly assumed to be $(\Omega,\mathcal A,\mathbb P)$ and $(H,\mathcal A, \mathbb P)$ for the Gaussian free field $h$.
	\item $(f,g)$ for the $L^2$ inner product $\int_D f(x)g(x)dx$ on some space $D$ which is usually clear from context; by abuse of notation (justified by Riesz) we will also write use this notation for more general objects like distributions/linear functionals that are not in $L^2$.
	\todo{that's kinda the wrong way round, actually we can define the standard inner product by integration by parts and the inverse Laplacian}
	\item $(f,g)_\nabla$ for the Dirichlet inner product $(\nabla f,\nabla g)$; again, by abuse of notation (justified by integration by parts), we will use it also for more general elements $f,g$ for which $(f,g)_\nabla = -(f,\Delta g)$ is well-defined.
	\item Write $H(D)$ (or often just $H$ and $H^1(D)$ to highlight it is a Sobolev space) for the Hilbert space completion of $H_s(D)$, the space of smooth functions, compactly supported in $D$, with respect to $(\cdot,\cdot)_\nabla$.
\end{itemize}

\section{Gaussian Free Field}

Prior to defining the GFF we will motivate it with some standard results from Gaussian Processes.

\subsection{Gaussian Processes}

\begin{defi}[Gaussian Process]
	A real valued stochastic process $(X(t):t\in T)$, where $T$ is some index set, whose finite dimensional distributions $\mu_F\deq \mathcal L((X(t):t\in F))$ are multivariate normal on $\mathbb R^F$ for all $F\subseteq T$ finite.
\end{defi}

\begin{rem}
	For historical reasons this is called $T$ like time, but it does not have to be $\mathbb R$. In fact, in what follows we will use a Hilbert space for $T$.
\end{rem}

Given that we did not impose any conditions on $T$ it is natural to ask for conditions under which such a process exists and whether it is unique. To do that we recall the that definition of a covariance on $T$ is a mapping $\Phi:T\times T\rightarrow\mathbb R$ such that we have for any $t_1,\dots, t_n\in T$ that the matrix $(\Phi(t_i,t_j))_{i,j=1}^n$ is symmetric and non-negative definite for all $n\in\mathbb N$.

\begin{lem}[Existence]\label{lem:existenceofGPgivenCov}
	Let $\Phi$ be a covariance on $T$, $f:T\rightarrow\mathbb R$ be a measurable function, then there exists a Gaussian process $(X(t):t\in T)$ such that for all $s,t\in T$ we have
	\begin{itemize}
		\item $\mathbb E X(t) = f(t)$
		\item $\mathbb E[(X(s)-f(s))(X(t)-f(t))]=\Phi(s,t)$
	\end{itemize}
\end{lem}

\todo{check if Kolmogorov's consistency theorem gives the kind of uniqueness we want for the proof in the SLE part}
%\begin{rem}[Uniqueness]
%	Realising $X$
%\end{rem}

\begin{rem}[Identification of Gaussian Processes and Gaussian Random Variables]\label{rem:GPandGrvs}
	It can be shown that for sufficiently nice metric spaces $T$ a Gaussian process $X:\Omega\times T\rightarrow\mathbb R$ naturally induces a Borel random variable $X':(\Omega,\mathcal A)\rightarrow (C_u(T,d),\mathcal B_{(C_u(T,d)})$ in the separable Banach space of $C_u(T,d)$, the space of uniformly continuous (w.r.t. the metric $d$) functions on $T$, by looking at the law of $X'(\omega)\deq X(\omega,\cdot)\in\mathbb R^T$.
	This result formalizes the intuitive notion of a Gaussian process being a ``random function''. The non-trivial parts of the claim are that it can be realised as a \emph{Borel} random variable (which is the natural topology on the target space; a priori Kolmogorov's extension theorem would only give us that elements of the cylinder-$\sigma$-algebra are measurable) and that the target space is \emph{separable}.
\end{rem}

While those assumptions will not hold in our more general setting, it should at least motivate an (informal) correspondence between Gaussian processes and Banach space valued Gaussian random variables as in the following definition, motivated by the characterisation of a Gaussian in $\mathbb R^d$ by its projections on any fixed $w\in\mathbb R^d$ which can, by duality, also be interpreted as a linear functional:

\begin{defi}[Banach Space Valued Gaussian Random Variables]
	For a separable Banach space $(B,\|\cdot\|_B)$ we say that $X:(\Omega,\mathcal A,\mathbb P)\rightarrow(B,\|\cdot\|_B)$ is Gaussian/normal whenever $f(X)$ is normally distributed for all $f\in B^*$.
\end{defi}

In the following our $T$ will not be ``small'' enough for some standard results from Gaussian processes, however, in some sense the probabilistic information about a Gaussian random variable in a separable Banach space is also encoded in its dual process:

\begin{defi}[Dual Process]
	Given some random variable $X$ in a separable Banach space $(B,\|\cdot\|_B)$ define its dual process $(\tilde X(f): f\in B^*)$ by the application $\tilde X(f)\deq f(X)$.
\end{defi}

\begin{rem}
	For example it can be shown that there exists a dense subset $D\subset B^*$ with $\|X\|_B = \sup_{f\in D}|\tilde X(f)|$.
\end{rem}

\begin{rem}
	Note that this dual process $\tilde X$ is actually a Gaussian process (check that its finite dimensional distributions are \emph{jointly} Gaussian by linearity of the functionals).
\end{rem}

Referring to the finite dimensional case again, we would like to generalize the fact that Gaussians in $\mathbb R^d$ can be written as $\sum_{i=1}^d e_ig_i$, where $(e_i)_i$ is any orthonormal basis of $\mathbb R^d$ and $g_i\iidnormal$, for (separable) Banach space valued Gaussian random variables. To do that we have to find a suitable subspace $H$ of the Banach space $B$ and endow it with the right Hilbert space structure to make sense of an orthonormal basis.

It turns out that reproducing kernel Hilbert spaces (henceforth abbreviated by RKHS) are the right choice:

\begin{defi}[Reproducing Kernel Hilbert Space for Gaussian Random Variables]
	Let $X:\Omega\rightarrow B$ be a Gaussian random variable in a separable Banach space $B$, $F\deq\{f(X):f\in B^*\}\subseteq L^2(\mathbb P)$, then the RKHS of $X$ is the Hilbert space $$H\deq\{\phi_h\deq\mathbb E(hX): h\in\overline F^{L^2(\mathbb P)}\},$$
	endowed with the inner product
	$$(\phi_h,\phi_g)_H\deq\mathbb E(hg),$$
	where the $\mathbb E$ in the definition of $H$ is the Bochner integral and $H\subseteq B$ by definition of the Bochner integral.
\end{defi}
\begin{rem}
	It can easily be seen that this definition coincides with the usual definition of a RKHS for Gaussian processes if the Banach space valued random variable is induced by some Gaussian process as in remark \ref{rem:GPandGrvs}.
\end{rem}

\begin{rem}[RKHS of Brownian Motion]\label{rem:RKHSofBM}
	Considering Brownian motion as a Banach space valued random variable as in remark \ref{rem:GPandGrvs}, it can be shown that the RKHS $H$ of the Wiener process on $T=[0,1]$ is $$H = \{h:[0,1]\rightarrow\mathbb R, h'\in L^2([0,1]), h(0)=0\},$$ with inner product induced by the usual Sobolev space norm $$\|h\|_H\deq \|h'\|_{L^2([0,1])}.$$
	Note that this is actually a norm because of the fixed boundary condition $h(0)=0$.
\end{rem}

Taking, for example, the trigonometric basis of $L^2([0,1])$, we get (by taking antiderivatives) an orthonormal basis $h_0(t)\deq t, h_k(t)\deq \frac{\sqrt 2}{\pi k}\sin(\pi kt)$ of $(H,(\cdot,\cdot)_H)$. Knowing that one can write Brownian motion as $$B(t)=\sum_{k=0}^\infty h_kg_k,$$ with $g_k\iidnormal$ one might expect this to hold in more general cases.
Indeed we have the following theorem:

\begin{thm}[Karhunen-Lo\`eve Expansion]
	Let $X$ be a centered (i.e. $\mathbb E f(X)=0 \forall f\in B^*$) Gaussian random variable in a separable Banach space $B$, $(h_k)_k$ an orthonormal basis of the RKHS $(H,(\cdot,\cdot)_H)$ of $X$, $g_k\iidnormal$, then
	$$X=\sum_k h_kg_k,$$
	almost surely with convergence of the sum in $B$.
\end{thm}

\begin{rem}[$X$ is in its RKHS iff dim $H<\infty$]\label{rem:XnotinRKHS}
	Note that $\mathbb E\|X\|_H^2 = \sum_{k=1}^{\text{dim }H}\mathbb E h_k^2$ by the previous theorem and absolute convergence of sum and $\mathbb E h_k^2 = 1$ for all $k$. Hence, by the $0-1$ law for Gaussian processes ($\mathbb E\|X\|_H^2=\infty \Rightarrow \exists\Omega_0\subseteq\Omega: \mathbb P(\Omega_0)>0$ on which $\|X(\omega)\|_H^2=\infty$), since $H$ is closed (hence measurable\footnote{Closed subsets $C$ of a topological space equipped with its Borel-$\sigma$-algebra $\mathcal B$ are $\mathcal B$-measurable.} in $B$, we see that $$\mathbb P(X\in H)=0$$
	if and only if $\text{dim }H=\infty$.
\end{rem}

\begin{rem}[$X$ as distribution]\label{rem:spaceofX}
	Now given the RKHS $H$ it is not immediate in what space $B$ the random variable $X$ (induced by the Karhunen-Lo\`eve expansion, i.e. $X\deq \sum_k h_kg_k$ using the notation from the theorem) will be. Indeed, this choice is rather arbitrary in general, as can be seen in the first section of \cite{She07} about abstract Wiener spaces.
	
	However, we may, informally, regard $B$ as a subset of a ``dual space'' $H'$, which is defined as the set of all $h=\sum_{k=1}^{\text{dim }H}h_ke_k$ (where $e_k$ is an \emph{ordered} orthonormal basis of $H$) with the property that the partial sums$\sum_{k=1}^n h_kf_k$ converge in $\mathbb R$ for every $f=\sum_{k=1}^{\text{dim }H}f_ke_k$.
	
	The reason to require a fixed ordering is the following calculation, where $h^n\deq \sum_{k=1}^n h_ke_k$ and $g_k\iidnormal$:
	
	\underline{$L^2$ convergence:}
	\begin{equation}\label{eq:varianceofHasRandomDistribution}
		\mathbb E(h^n,f)_H^2 \eqbydef \mathbb E\left(\sum_{k=1}^n g_k f_k\right)^2 \eqby{g_k\iidnormal} \sum_{k=1}^n f_k^2
	\end{equation} converges against $\|f\|_H^2$.
	
	\underline{Almost sure convergence:} $L^2$ convergence implies convergence in probability, which in turn, by Levy's equivalence theorem (the $g_k$ are independent), implies almost sure convergence of the \emph{partial sums} given a fixed ordering.
	
	This restriction is not artificially introduced by a lack of means to show the stronger result of $X\eqbydef \sum_k h_kg_k$ being in the topological dual space $H^*$ since this would, by Riesz, imply that $X$ is actually in $H$ - a contradiction to remark \ref{rem:XnotinRKHS} in our infinite dimensional setting.
	
	However, it can be shown (as in \cite{She07}) that $X$ is indeed a distribution, i.e. a continuous (with respect to the topology of uniform convergence of all derivatives) linear functional on the space of smooth compactly supported functions $H_s$. By the above remark its continuity cannot be preserved when extending it to the larger space $H^1$ where it is only a linear functional.
	
	Note that (in two dimensions, i.e. $D\subseteq\mathbb R^2$) there is in fact a unique extension since $h$ is not only a distribution, but in a sense ``arbitrarily close'' to being in $L^2$\todo{check that this L2 thing is enough to extend it to H and not only any distribution}, i.e. $h\in (-\Delta)^\eps L^2(D)$ for all $\eps>0$. Naturally continuity on a dense subset (here $H_s$ dense in $H^1$) alone does not suffice for a well-defined extension to $(H^1)^*$ - take for example $\nabla\delta_0$ and see that it cannot be well defined for $f\in H$ since the derivative of $f$ only needs to be in $L^2$ which is not defined pointwise.
\end{rem}

While we \emph{could} define the Gaussian free field that way, i.e. as a sum of orthonormal basis elements weighted by iid normals, it has the aesthetic drawback of being somewhat arbitrary and a priori not being well defined for general $f\in H(D)$, but only for $f\in H_s(D)$. Instead we will be slightly less explicit and define it to be the dual process of $X$ as described in remark \ref{rem:spaceofX} with the following adaption of the RKHS of the Wiener process: Noting that Brownian motion on $[0,1]$ is induced by the Sobolev space $H^1([0,1])$ it seems natural to investigate the random variable induced by the RKHS $H^1(D)$ for some domain $D\subseteq\mathbb R^d$. An easy (formal) calculation (just like \eqref{eq:varianceofHasRandomDistribution}, but for $\mathbb E(h^n,f)_H(h^n,g)_H$) for $X$ as above motivates the covariances for the dual process as given in the following definition:

\begin{defi}[Gaussian Free Field]
	The Gaussian free field on a domain $D\subseteq\mathbb R^d$ is the centered Gaussian process $$h\deq ((h,f)_\nabla : f\in H(D))$$ with covariance structure $$\mathbb E[(h,a)_\nabla(h,b)_\nabla]\deq (a,b)_\nabla.$$
\end{defi}
Note that this is well-defined by lemma \ref{lem:existenceofGPgivenCov}.
\begin{rem}
	It follows easily from the definition that $f\mapsto (h,f)_\nabla$ is linear and that for every $f\in H(D)$ we have that $(h,f)_\nabla\sim\mathcal N(0,\|f\|_{H(D)})$.
\end{rem}

\begin{rem}
	Note that Dirichlet boundary conditions are ``hidden'' in the vanishing boundary conditions on elements in $H(D)$. %Furthermore we will assume that $h$ is zero outside of $D$ (which will take care of technical problems when averaging over circles near the boundary later on)
\end{rem}From now on we will restrict ourselves to the case $D\subseteq\mathbb R^2$. For what follows it will be useful to define the Green's function $G_D$ of a domain $D$ and stating some of its basic properties:

\begin{defi}[Green's function]
	For $x\in D$ fixed we let $$G_D^x(\cdot)\deq -\log|x-\cdot|-\tilde G_D^x(\cdot),$$ where $\tilde G_D^x$ is the unique harmonic function chosen such that $G_D^x(y)=0$ for all $y\in\partial D$.
	If it is clear from context what $D$ is we will drop it and simply write $G^x$ for $G^x_D$.
	To emphasise symmetry we will also sometimes write $G(x,y)$ for $G^x(y)$.
	
	We also define the corresponding integral operator $$[-\Delta^{-1}\rho](x)\deq\int_D G(x,y)\rho(y)dy,$$ which is the inverse of $-\Delta$.
\end{defi}

\begin{rem}[Importance of Green's Function]\label{rem:usesofGreensfct}
	One of the reasons the Green's function is so important is that it can be seen as the distributional solution of $-\Delta G^x = \delta_x$. 
	Noting that by viewing $h$ as a distribution we would immediately get\footnote{The derivative of a distribution (motivated by integration by parts) is defined by $(\nabla h,\rho)\deq(h,-\nabla\rho)$ for smooth test functions $\rho$.} $(h,\rho)\eqbydef -(h,\Delta^{-1}\rho)_\nabla$ for smooth compactly supported $\rho$  we just take (equivalently to viewing $(h,f)_\nabla$ as well defined for all $f\in H$) $(h,\rho)\deq -(h,\Delta^{-1}\rho)_\nabla$ to be well-defined for all $\rho$ that can be written as $\rho = -\Delta f$ for some $f\in H(D)$.
\end{rem}

\begin{rem}[Qualitative Difference of GFF in Different Dimensions]
	One might wonder why a Wiener process, which can be seen as a Gaussian free field on $[0,1]$, is a H\"older continuous function while the Gaussian free field on any domain $D\subseteq \mathbb R^2$ is not even a function (i.e. does not have values at points), but merely a distribution.
	
	Firstly, we note that this is not as surprising as it may seem if we (equivalently!) chose to introduce Gaussian free fields as in remark \ref{rem:spaceofX} since H\"older continuous functions are not in the RKHS anymore as well.
	
	Furthermore remark \ref{rem:usesofGreensfct} suggests that the different behaviour of the Green's function in different dimensions is the reason\footnote{One might also present a different argument, as in \cite{She07}, arguing with the qualitatively different decays of eigenvalues of the Laplacian in different dimensions.} we cannot define the GFF pointwise for dimensions $d\geq 2$: Assume we could, then make the ansatz: $h(x)=(h,\delta_x)\eqbydef -(h,\Delta^{-1}\delta_x)_\nabla = -(h,G^x)_\nabla.$ Now it is easy to see\footnote{Note that we only defined the Green's function in dimension two, but it can be defined more generally in arbitrary dimensions as the distributional solution of $-\Delta G^x=\delta_x$ so that the above ansatz makes sense.} that while $G^x\in H([a,b])$, it fails to be in $H(D)$ in dimensions $d\geq 2$ (i.e. for $D\subseteq\mathbb R^2$).
\end{rem}

We recall the following properties of Green's functions in $\mathbb C$:

\begin{pro}[Properties of $G_D^x$]\label{prop:propertiesofGreensfcts} For $D\subseteq\mathbb C$ fixed and $\tilde G^x$ as in the definition we have
	\begin{enumerate}
		\item $G(x,y) = G(y,x)$ for all $x,y\in D\setminus\{x\}$. (symmetry)
		\item $\Delta G_D^x(y)=0$ for all $y\in D\setminus\{x\}$, i.e. it is harmonic in $D\setminus\{x\}$. (harmonicity)
		\item\label{item:GreensfctatXisconfRadius} $\tilde G^x(x)=\log C(x;D)$ for all $x\in D$.
	\end{enumerate}
\end{pro}

\subsection{Properties}

%\subsubsection{Covariances}

%Writing $[-\Delta^{-1}\rho](x) = \int_D G^x(y)\rho(y)dy$ easily gives the following proposition

%\begin{pro}
%	$$\mathbb E[(h,\rho_1) (h,\rho_2)]=\int_{D\times D}\rho_1(x)G(x,y)\rho_2(y)dxdy,$$
%for all $\rho_1,\rho_2 \in (-\Delta)H(D)$.
%\end{pro}

\subsubsection{Markov Property}

\begin{rem}[Motivation by Brownian Motion]\label{rem:alternativeMarkovforBM}
	The Markov property of Brownian motion can also be stated as follows: Given a Brownian motion $B=(B_t)_{t\geq 0}$ then $B$ is equal in distribution to $B^{a,b}$ which is obtained by sampling a Brownian motion $B$, an independent Brownian bridge $\tilde B$ on $(a,b)$ (i.e. a Brownian motion conditioned to be zero on $\mathbb R^+\setminus(a,b)$), and taking
	$$B^{a,b}_t\deq \begin{cases}
      B_t, & \text{if}\ t\in\mathbb R^+\setminus(a,b) \\
      b_t+\tilde B_t, & \text{if}\ t\in(a,b)
    \end{cases}$$ where $(b_t)_{t\in(a,b)}$ linearly interpolates $B_a$ and $B_b$.
	
	This can be rephrased in a slightly more abstract (and less rigorous) way as ``given the values of a Brownian motion $B$ outside some open set $U=(a,b)$, the values in $U$ are equal in distribution to the projection on the harmonic part\footnote{Note that in one dimension the harmonic functions are just linear functions.} plus an independently sampled Brownian motion with zero boundary conditions on $(a,b)$''.
\end{rem}

We hope for something similar to hold for Gaussian free fields in two dimensions (with zero boundary conditions) as well. To make this rigorous we first have to define what we mean by the ``projection on the harmonic part'', so we define the following subspaces of $H=H(D)$:

\begin{defi}
	For $U\subseteq D$ open we write \begin{itemize}
		\item $H_U$ for the closure (with respect to $(\cdot,\cdot)_\nabla$) of the set of smooth functions supported in some compact subset of $U$ and
		\item $H_U^\perp\deq \{b\in H: \Delta b=0 \text{ on }U\}$, the functions which are harmonic on $U$.
	\end{itemize}
\end{defi}

\begin{rem}[Justification of the Definition]
	Since $U$ is assumed to be open integration by parts\footnote{We need $U$ open since otherwise it could be that the intersection of a compact subset of $U$ and $\partial U$ is not empty, so $a\in H_U$ would not imply that $a$ has zero boundary conditions - hence introducing an additional (potentially non-zero) term when integrating by parts.} implies that for $a\in H_U, b\in H_U^\perp$ we have $$\mathbb E[(h,a)_\nabla(h,b)_\nabla]\eqbydef (a,b)_\nabla = -(a,\Delta b)=0,$$
	so the two spaces are indeed orthogonal.
\end{rem}

\begin{rem}[Independence of Orthogonal Subspaces]
	Note that, by definition of the covariance function for the Gaussian free field $\mathbb E[(h,a)_\nabla(h,b)_\nabla]\eqbydef(a,b)_\nabla$ and the fact that vanishing covariance implies independence for Gaussians, we have that $(h,\cdot)_\nabla$ restricted to orthogonal subspaces $H_1(D),H_2(D)\subseteq H(D)$ are independent of one another. Moreover, if $H_1(D),H_2(D)$ span $H(D)$ then $\mathcal F_{H_1},\mathcal F_{H_2}$, the smallest $\sigma$-algebras such that $(h,f)_\nabla$ are measurable for all $f$ in $H_1$,$H_2$ respectively, generate $\mathcal F$, the smallest $\sigma$-algebra making $(h,\cdot)_\nabla$ measurable. 
\end{rem}

\begin{thm}[Markov Property of the GFF]\label{thm:MarkovProperty}
	The two subspaces $H_U(D), H_U^\perp(D)$, defined as above, span $H(D)$.
\end{thm}

This theorem tells us that we can write every $f\in H(D)$ as sum of elements from $H_U$ and $H_U^\perp$, which is, by the previous remark, analogous to the alternative statement of Brownian motion given in remark \ref{rem:alternativeMarkovforBM}.

\begin{proof}
	By a density argument it suffices to show that every $f\in H_s(D)$, the space of smooth functions compactly supported in $D$, can be written as $a+b$, with $a\in H_U(D), b\in H_U^\perp$.
	
	\underline{The idea:} Intuitively we would like to set $b$ to be the unique continuous function which equals $f$ outside of $U$ and is harmonic inside of $U$, and then set $a=f-b$. However, if for example $U$ is the complement of a single point $z\in D$, then there is no unique $b$ which coincides with $f$ on $\{z\}$ and is harmonic on $U$. This is similar to trying to solve the Laplace equation in one dimension given only the initial value, but no derivatives which would be defined if we were ``allowed'' to know the values of $f$ in a neighbourhood of $z$ since $f\in H_s(D)$. This motivates the following approach:
	
	\underline{The problem:} We take the $\delta$-neighbourhood of the complement of $U$ (so that every point in the complement of $U$ gets a ``padding'' of $\delta$), take its complement $U_\delta\subseteq U$, and use the fact - well-known from probability - that the expectation of a function evaluated at the first exit point of Brownian motion of a set is harmonic in that set and (by definition) equal to the original function outside of that set.
	
	\underline{The solution:} More precisely, we take $$b_\delta(x)\deq \mathbb E_x(f(B_{\tau(\delta)})),$$ where $\tau(\delta)$ is the first time $t$ the Brownian motion $B_t$ exits $U_\delta$. Then $a_\delta\deq f-b_\delta$ is supported on a compact subset of $U$ and is in $H_{U_\delta}$, which is increasing (as $\delta\searrow 0$) family of subspaces of $H_U$. Now $a_\delta$ converges to some $a\in H_U$. Thus the $b_\delta$ must converge to some $b$ which has to be in $H_U^\perp$ since the $\|\cdot\|_\nabla$ limit of harmonic functions is harmonic (just like the $L^2$ limit of constant smooth functions is constant). 
	We still have $f=a+b$ which finishes the proof.
\end{proof}

\subsubsection{Circle Averages}

Although $h$ is not a function (i.e. it does not have values at points), we shall see circle averages are well-defined by duality. To see this first introduce the $\eps$-regularized Green's function:

\begin{defi}
	We let
	$$G_{D,\eps}^x(\cdot)\deq -\log[|x-\cdot|\vee \eps]-\tilde G_{D,\eps}^x,$$
	where, as before, $\tilde G_{D,\eps}^x$ is the unique harmonic (in $D$) function such that $G_{D,\eps}^x(y)=0$ for all $y\in \partial D$. In case it is clear from context what $D$ is we will also drop it and simply write $G_\eps^x$ for $G_{D,\eps}^x$.
\end{defi}

Now this is harmonic in $B_\eps(x)$ (since it is and does not have a singularity anymore) and in $\overline{B_\eps(x)}^c$ (as a sum of two harmonic functions) so its Laplacian will be supported on $\partial B_\eps(x)$. So one might expect that $-\Delta G_\eps^x$ is some kind of uniform measure on $\partial B_\eps(x)$. 

Indeed, using the rotational symmetry and lack of (classical) differentiability at $\partial B_\eps(x)$ of the derivative, the following is easy to see:

\begin{pro}\label{prop:circleAvg}
	for any $x\in D, \eps>0$ with $\eps$ small enough\footnote{$\eps$ such that $\eps<d(x,\partial D)$} we have that 
	\begin{enumerate}
		\item $G_\eps^x\in H^1(D)$
		\item $-\Delta[G_\eps^x] = \nu_\eps^x$, where $\nu_\eps^x$ is the uniform measure of total mass one on $\partial B_\eps(x)$.
	\end{enumerate}
\end{pro}

While (1) ensures that $(h,G_\eps^x)_\nabla$ is well defined, (2) states that $(h,G_\eps^x)_\nabla = (h,\nu_\eps^x)$ can be interpreted as a circle average of $h$ with radius $\eps$ around $x$.
\begin{proof}
	Write it in polar coordinates and use that $\Delta = \frac{\partial^2}{\partial r^2}+\frac{1}{r}\frac{\partial}{\partial r}+\frac{1}{r^2}\frac{\partial^2}{\partial\theta^2}$, where the last term vanishes by rotational symmetry.
\end{proof}

Thus the following definition is well-posed and can be interpreted as circle average:

\begin{defi}[$\eps$-regularized GFF $h_\eps$]
	$$h_\eps(z)\deq (h,G_\eps^z)_\nabla$$
\end{defi}

Since working directly with $h$ is not possible in what is to come we will want to approximate $h$ with $h_\eps$ by letting $\eps$ tend to zero. In those calculations it will turn out to be helpful to see that for any fixed $z\in D$ and suitably chosen $t_0$ the process $(h_{e^-t}(z))_{t\geq t_0}$ is a standard Brownian motion.

\begin{lem}[Circle Averages Are Brownian Motion as Radius Decreases]\label{lem:hepsIsBM}
	Let $h$ be a Gaussian free field in $D$, $z\in D$ fixed, $t_0\deq\inf\{t\geq 0: B_{e^{-t}}\subseteq D\}$, and $$\mathcal B_t\deq h_{e^{-(t+t_0)}}(z)-h_{e^{-t_0}}(z).$$
	Then $(\mathcal B_t)_{t\geq 0}$ is a standard Brownian motion.
\end{lem}

Before we start with the proof of the lemma we first express the covariances of the $\eps$-regularized Gaussian free field $h_\eps$ in terms of the conformal radius $C(z;D)$ of $D$ viewed from $z\in D$ which is defined as $C(z;D)\deq \phi'(z)^{-1}$, where $\phi:D\rightarrow\mathbb D$ is the unique\footnote{This is well-defined by the Riemannian mapping theorem.} conformal map from $D$ to $\mathbb D$ mapping $z$ to $0$ such that $\phi'(z)\geq 0$.

\begin{pro}[Covariance Structure of $h_\eps$]\label{prop:covariancesOfRegularizedH}
	Writing $$G_{\eps_1,\eps_2}(z_1,z_2) \deq \mathbb E[h_{\eps_1}(z_1)h_{\eps_2}(z_2)] = \int_{D\times D} G^x(y)d\nu_{\eps_1}^{z_1}(x)d\nu_{\eps_2}^{z_1}(y),$$ for $\nu_\eps^z$ the uniform measure of total mass one on $\partial B_\eps(z)$ as before, we have
	\begin{enumerate}
		\item $G_{\eps_1,\eps_2}(z_1,z_2) = (G_{\eps_1}^{z_1},G_{\eps_2}^{z_2})_\nabla = (G_{\eps_1}^{z_1},\nu_{\eps_2}^{z_2})$, the mean value of $G_{\eps_1}^{z_1}$ on $\partial B_{e_2}(z_2)$.
		\item If $B_{\eps_1}(z_1)$ and $B_{\eps_2}(z_2)$ are disjoint and in $D$, then $$G_{\eps_1,\eps_2}(z_1,z_2) = G(z_1,z_2),$$ the usual (i.e. not regularized) Green's function.
		\item\label{item:covarianceForSameZdifferentEps} If $B_{\eps_1}(z)\subseteq D$ and $\eps_1\geq\eps_2$ then $$G_{\eps_1,\eps_2}(z,z)=-\log\eps_1+\log C(z;D).$$
	\end{enumerate}
\end{pro}

\begin{proof}[Proof of proposition]
	(1) follows directly by definition and partial integration, using proposition \ref{prop:circleAvg}.
	
	(2) follows by the circle average property of harmonic functions, using that $G^x$ is harmonic in $D\setminus\{x\}$ and coincides with $\tilde G^x_\eps$ in $D\setminus B_\eps(x)$.
	
	(3) holds by the following calculation:
	\begin{align*}
		G_{\eps_1,\eps_2}(z,z)&\eqby{(1)}(G_{\eps_1}^{z_1},\nu_{\eps_2}^{z_2})\\
		&\eqbydef \int_\mathbb{C}(-\log(\eps_1\vee |z-y|)-\tilde G_{\eps_1}^z(y))d\nu_{\eps_2}^z(y),
	\end{align*}
	where the first summand equals $-\log\eps_1$ since, by definition of $\nu_{\eps_2}^z$, we integrate only over $y\in\partial B_{\eps_2}(z)$ and $\int_\mathbb{C}d\nu_{\eps_2}^z(y)=1$, and the second summand evaluates to $\tilde G^z(z)$ by the circle average property of harmonic functions and because $\eps_1\leq\text{dist}(z,\partial D)$ (so that the harmonic correction term does not ``see'' the change around $z$). Using proposition \ref{prop:propertiesofGreensfcts}(\ref{item:GreensfctatXisconfRadius}) this is exactly what we wanted to show.
\end{proof}

\begin{proof}[Proof of lemma]
	Clearly $(\mathcal B_t)_{t\geq 0}$ is a centered Gaussian process. Hence it suffices to show that for any $0\leq s\leq t$ we have that $\mathbb E[\mathcal B_s\mathcal B_t] = s$. 
	
	By definition we have $$\mathbb E[\mathcal B_s\mathcal B_t]\eqbydef \mathbb E[(h_{e^{-(s+t_0)}}(z)-h_{e^{-t_0}}(z))(h_{e^{-(t+t_0)}}(z)-h_{e^{-t_0}}(z))],$$ so using linearity of expectation and proposition \ref{prop:covariancesOfRegularizedH}(\ref{item:covarianceForSameZdifferentEps}) repeatedly one easily arrives at $\mathbb E[\mathcal B_s\mathcal B_t] = s$.
\end{proof}

Note that one could have done the same calculations with the $\eps$ parameterization to get some logarithmic terms in the end (instead of the correct $s$) - this way it would become obvious why we chose the $e^{-t}$ parameterization.

\section{Liouville Measures}

In this section we want to study two dimensional random surfaces. Similarly to how Brownian motion (the one dimensional Gaussian free field) is a ``random line'', a Gaussian free field in some bounded domain $D\subseteq\mathbb C$ can be identified with a random Riemann surface via the correspondence suggested by the following theorem:
\begin{thm}[Riemann uniformization theorem]
	For every smooth, simply connected Riemannian manifold\footnote{Note that a surface can be described by several quantities like the area of any fixed measurable subset, its geodesics, the length of any fixed, smooth curve, etc. Here we will focus on the area.} $(\mathcal M,\mu)$ there exists a real valued function $\lambda:D\rightarrow\mathbb R$ for $D\in\{\mathbb C,\mathbb C\cup\{\infty\},\mathbb D\}$ such that the Radon Nikodym derivative of $\mu$ at $z\in D$ is given by $e^{\lambda(z)}$.
\end{thm}

\subsection{Existence of Weak Limits}

Now we would like to take $\lambda = \gamma h$ (for some fixed parameter $\gamma\geq 0$) to get two dimensional random surfaces, but since $h$ does not have values at points this is not well-defined. Instead, we will take $\lambda = \gamma h_\eps$ and see ``in which order of magnitude this approximation typically explodes'' as $\eps$ goes to zero - or to be more precise: Find $c,\alpha$ such that $\mathbb E e^{\gamma h_\eps}(z) \rightarrow c\eps^\alpha$.

Normalising by this factor, i.e. taking $\lambda=\gamma h_\eps - \alpha\log\eps$, so that $\mathbb E e^{\gamma h_\eps(z) - \alpha\log\eps}=O(1)$, will be our best shot at finding a well defined limiting measure.

\begin{rem}[Asymptotics of $\mathbb E e^{\gamma h_\eps(z)}$]
	Noting that $\mathbb E e^{\mathcal N(a,b)}=e^{a+b/2}$ and $h_\eps(z)\sim\mathcal N(0,-\log\eps+\log C(z;D))$ by \ref{prop:covariancesOfRegularizedH}(\ref{item:covarianceForSameZdifferentEps}) we get that $$\mathbb E e^{\gamma h_\eps(z)} = e^{\gamma^2/2(-\log\eps+\log C(z;D))}=\left(\frac{C(z;D)}{\eps}\right)^{\gamma^2/2} = c\eps^{-\gamma^2/2}.$$
	So it seems natural to normalise by a factor of $\eps^{\gamma^2/2}$:
\end{rem}

\begin{defi}[Regularized $h_\eps$]
	Write $\overline h_\eps\deq \gamma h_\eps+\frac{\gamma^2}{2}\log\eps$.
\end{defi}


Intuitively a weak limit should be strong enough to preserve information about area since (at least for probability measures) we have $\mu_n\rightarrow\mu$ weakly implies that $\mu_n(B)\rightarrow\mu(B)$ for all $B$ in the Borel-$\sigma$ algebra with $\mu(\partial B)=0$.

\begin{thm}[Weak Limit of $\eps^{\gamma^2/2}e^{\gamma h_\eps}$]\label{thm:weaklimitofmueps}
	Let $h$ be a Gaussian free field in some bounded domain $D\subseteq\mathbb C$, $\gamma \in [0,2)$ fixed, and $\mu_\eps\deq\eps^{\gamma^2/2}e^{\gamma h_\eps(z)}dz = e^{\overline h_\eps(z)}dz$. Then $\mu_\eps$ converges weakly almost surely against some $\mu=\mu_h$ for $\eps=2^{-k}\rightarrow 0$ as $k\rightarrow \infty$.
\end{thm}
\begin{proof}
	\underline{Reduction to squares:} It suffices to show that $\mu_{2^{-k}}(S)$ converges almost surely against some finite limit $\mu(S)$ for every diadic square compactly supported in $D$.
	
	To see this note that there are only countably many such squares $\{S_i\}_{i\in\mathbb N}$, so if we show that $\mu_{2^{-k}}(S_i)\rightarrow\mu(S_i)$ on $\Omega_i$ with $\mathbb P(\Omega_i)=1$, then this still holds almost surely for all squares at once. Now we want to show that $\mu_{2^{-k}}(f)\rightarrow\mu(f)$ on some $\Omega_0$ with $\mathbb P(\Omega_0)=1$ for all $f\in C_b(D)$. To see that, note that $\mathcal S\deq\{\sum_{i=1}^N a_i 1_{S_i}:N\in\mathbb N, a_i\in\mathbb R, S_i \text{ disjoint, diadic squares}\}$ are dense in $L^1(\mu_{2^{-k}})$ (hence in particular in $C_b(D)$ as $\mu_{2^{-k}}$ is a Radon Nikodym derivative of the Lebesgue measure, so continuous functions are measurable) just as simple functions. So for $f_N = \sum_{i=1}^N a_i^N1_{S_i^N}\in\mathcal S$ such that $f_N\rightarrow f$ in $L^1(\mu_{2^{-k}})$ almost surely\footnote{Note that it is important to look at the \emph{countable} family of diadic squares.} we can write
	\begin{align*}
		\mu_{2^{-k}}(f)&=\mu_{2^{-k}}\left(\lim_N\sum_{i=1}^N a_i^N1_{S_i^N}\right)\\
		&= \lim_N\sum_{i=1}^N a_i^N\mu_{2^{-k}}(S_i^N) =\lim_N \mu_{2^{-k}}(f_N)
	\end{align*}
	Taking $k\rightarrow\infty$ we only need to show that $$\lim_k\lim_N \mu_{2^{-k}}(f_N) = \lim_N\lim_k \mu_{2^{-k}}(f_N),$$
	to get that $\mu_{2^{-k}}(f)\rightarrow \mu(f)$ almost surely as $k\rightarrow\infty$.
	This is justified by the Moore-Osgood theorem if
	\begin{itemize}
		\item $\lim_N \mu_{2^{-k}}(f_N)=\mu_{2^{-k}}(f)$ uniformly for all $k<\infty$.\footnote{This notation should emphasize that $k=\infty$ need not be checked, even though it is part of the extended natural numbers $\mathbb N\cup\{\infty\}$ we are working with here. Also we write $f$ instead of $f_\infty$ and $\mu$ instead of $\mu_{2^{-\infty}}$.}
		\item $\lim_k \mu_{2^{-k}}(f_N)=\mu(f_N)$ for all $N$ fixed.
	\end{itemize}
	The first requirement follows almost surely since for all $c>0$ we have by almost sure $L^1$ convergence $$\mathbb P\left(\sup_{k<\infty}|\mu_{2^{-k}}(f_N)-\mu_{2^{-k}}(f)|\geq c \;\;\forall N\in\mathbb N\right)=0.$$
	The second follows by definition of infinite sums.
	Note that we could not use the Portmanteau theorem since we normalised the measures so that they are $O(1)$ as $\eps\rightarrow 0$; so while they are finite with probability one (since their expectation is), they need not be probability measures.

	
	\underline{Reduction to exponential decay:} To show that $\mu_{2^{-k}}(S)\rightarrow\mu(S)<\infty$ with probability one, we show that $\mathbb E|\mu_{2^{-(k+1)}}(S)-\mu_{2^{-k}}(S)|$ decays exponentially in $k$.
	
	Writing $X_k\deq \mu_{2^{-k}}(S)$, a real valued random variable, the exponential decay implies that $(X_k)_k$ is a Cauchy sequence in $L^1(\mathbb P)$, but the $L^1$ is complete and hence the limit $X=\mu(S)$ is in $L^1$ again. Since its expectation is finite it implies that $\mu(S)<\infty$ with probability one.
	
	\underline{Strategy to prove exponential decay:} \todo{choose better title for this subsection}\todo{where does it help to take the second moment instead of showing it directly for the first?}To show that $\mathbb E|\mu_{2^{-(k+1)}}(S)-\mu_{2^{-k}}(S)|$ decays exponentially in $k$ we assume without loss of generality\footnote{Otherwise we would have to keep writing an additional factor given by the (Lebesgue) area of $S$.} that $S=[0,1]^2$, so that $\mu_\eps(S)$ is just the mean value of $e^{\overline h_\eps}$ on $S$ and calculate $\mathbb E(|\mu_{2^{-(k+1)}}(S)-\mu_{2^{-k}}(S)|^2)$ (note that showing exponential decay of the second moment is enough since the first moment $\mathbb Ee^{\overline h_\eps(z)}=C(z;D)^{\gamma^2/2}$ is positive and bounded from below and above uniformly in $z\in S$ since we assumed that $S$ is compact in $D$ open and the conformal radius can be bounded from below and above by $\text{dist}(z,\partial D)$):
	
	We would like to formalize the intuition that, by the Markov property, $h_\eps(z)$ and $h_\eps(z')$, conditioned on $h_{2\eps}(z)$ and $h_{2\eps}(z')$ respectively, are independent (for $\eps$ small enough) which should make calculations easier. In the following we will take the average values of $\overline h_{2^{-(k+1)}}$ and $\overline h_{2^{-(k+2)}}$, conditioned on $\overline h_{2^{-(k+1)}}$, over some lattice chosen $S_k^y$ such that all those values are independent. Then we will average over all those lattices $(S_k^y)_{y\in S}$ and take another expectation to get rid of the conditioning.
	
	To make that rigorous we define \begin{itemize}
		\item $S_k^y\deq (y+2^{-k}\mathbb Z)\cap D$
		\item $A_k^y\deq \frac{1}{|S_k^y|}\sum_{z\in S_k^y}exp(\overline h_{2^{-(k+1)}}(z))$
		\item $B_k^y\deq \frac{1}{|S_k^y|}\sum_{z\in S_k^y}exp(\overline h_{2^{-(k+2)}}(z))$
	\end{itemize}
	the average values of $h_{2^{-(k+1)}}$ and $h_{2^{-(k+2)}}$ over $S_k^y$, respectively.
	
	\underline{Reduction to disjoint balls:} Writing $\mathbb E^y$ for the expectation over $y\in S$ (given by the Lebesgue measure) we have \begin{align*}
		\mathbb E|\mu_{2^{-k}}(S)-\mu_{2^{-(k+1)}}(S)| &= \mathbb E|\mathbb E^y(exp(\overline h_{2^{-k}})-exp(\overline h_{2^{-(k+1)}}))|\\
		&=\mathbb E|\mathbb E^y(A_k^y-B_k^y)\\
		&\leqby{\text{Jensen}}\mathbb E\mathbb E^y|A_k^y-B_k^y|\\
		&\eqby{\text{Tonelli}}\mathbb E^y\mathbb E|A_k^y-B_k^y|
	\end{align*}
	
	so it suffices to show that $\mathbb E|A_k^y-B_k^y|$ decays exponentially in $k$, uniformly for all $y\in S$. (Note that we can apply Jensen since $\mathbb E^y(1)=\int_S ds=1$ since we assumed that $S=[0,1]^2$. Otherwise we would have had to introduce a normalising factor.)
	
	\underline{Getting a conditional expectation:} Now by the Markov property (theorem \ref{thm:MarkovProperty}) of the Gaussian free field we have that the random variables $h_{2^{-(k+2)}}(z)$ conditioned on $h_{2^{-(k+1)}}(z)$ are independent of one another for different $z\in S_k^y$. Moreover, by lemma \ref{lem:hepsIsBM}, we have that they are $\mathcal N(h_{2^{-(k+1)}}(z),\log 2)$. Using these results an elementary (but tedious) calculation shows that
	\begin{align*}
		\mathbb E[|A_k^y-B_k^y|^2|\{h_{2^{-(k+1)}}(z)\}_{z\in S_k^y}] &\eqbydef 2^{-4k}\sum_{z\in S_k^y}\mathbb E[|e^{\overline h_{2^{-(k+1)}}(z)}-e^{\overline h_{2^{-(k+2)}}(z)}|^2|\{h_{2^{-(k+1)}}(z)\}_{z\in S_k^y}]\\
		&=2^{-4k}C\sum_{z\in S_k^y}\left(e^{\overline h_{2^{-(k+1)}}(z)}\right)^2,\numberthis \label{eq:conditionalExpOfAkBksquared}
	\end{align*}
	where $C>0$ is some constant only depending on $\gamma$.
	
	\underline{Showing exponential decay for the unconditional expectation:} Now we can take expectations to get the unconditional expectation $\mathbb E|A_k^y-B_k^y|^2$, but it turns out that the naive approach of bounding the expectation by the second moment only works for $\gamma<\sqrt 2$ since
	
	\begin{align*}
		\mathbb E[|A_k^y-B_k^y|^2] &\eqby{\eqref{eq:conditionalExpOfAkBksquared}}
		2^{-4k}C\sum_{z\in S_k^y}\mathbb E\left[(e^{\overline h_{2^{-(k+1)}}(z)})^2\right]\\
		&= O\left(|S_k^y|2^{-4k}2^{-k\gamma^2}\mathbb E[e^{2\gamma h_{2^{-(k+1)}}}]\right) \\
		&= O(2^{2k}2^{-4k}2^{-k\gamma^2}2^{2k\gamma^2})\\
		&= O(2^{-k(2-\gamma^2)}),
	\end{align*}
	which finishes the proof for $\gamma<\sqrt 2$ (because we have exponential decay in that regime).
	
	The more general result for $2\leq\gamma^2<4$ can be obtained by showing that the second moment is only made large by rare occurrences of $h_\eps(z)$ being much larger than expected and that their contribution to the first moment (the expectation) vanishes exponentially.
\end{proof}

\subsection{Parameterizing a Surface with Different Domains}
We will start by stating the main result of this subsection (mainly to introduce notation), then state some immediate applications, and conclude by proving some other results needed to show it.
\begin{pro}[Change of Coordinates]\label{prop:changeofcoordinates}
	Let $h$ be a Gaussian free field on a domain $D$, $\psi$ a conformal map from a domain $\tilde D$ to $D$, and $\tilde h\deq h\circ\psi+Q\log|\psi'|$ a distribution on $\tilde D$, where $Q\deq \frac{2}{\gamma}+\frac{\gamma}{2}$.
	
	Then $$\mu_{\tilde h}=\mu_h\circ\psi,$$
	i.e. $\mu_{\tilde h}(A)=\mu_h(\psi(A))$ for all Borel measurable $A\subseteq \tilde D$.
\end{pro}
\begin{rem}[Motivation]
	For some calculations we would like to parameterize the same surface given by the Gaussian free field on $D$ with a different domain $\tilde D$ since some quantities may be easier to calculate in one domain than in another. It also leads to a definition of a class of random surfaces henceforth referred to as quantum surfaces which will be shown to exhibit connections to SLEs later on.
\end{rem}

\begin{defi}[Quantum Surfaces]\label{def:quantumsurface}
	A quantum surface is an equivalence class of pairs $(D,h)$ under the equivalence transformation $$(D,h)\rightarrow \psi^{-1}(D,h)\deq (\psi^{-1}(D),h\circ\psi+Q\log|\psi'|)=(\tilde D,\tilde h).$$
\end{defi}

This definition admits the following interpretation: Pick a random surface $(D,h)$ induced (or ``parameterized'') by the Gaussian free field $h$ on $D$ and let $\tilde D$ be another domain with some conformal $\psi:\tilde D\rightarrow D$. Now if we wanted to consider the same surface parameterized by $\tilde D$ instead of $D$ proposition \ref{prop:changeofcoordinates} tells us to consider $\tilde h\deq h\circ\psi + Q\log|\psi'|$, the Gaussian free field on $\tilde D$ (by conformal invariance) plus some correction term to account for the local changes of $\psi$.

\begin{rem}[Preliminaries for the Proof]
	First we recall that $(\cdot,\cdot)_\nabla$ is conformally invariant in two dimensions and that, by the Cauchy-Riemann equations, we have that the determinant of a Jacobian for a conformal coordinate change $\psi$ is given by $|\psi'|^2$.
\end{rem}
\begin{proof}[Proof of the proposition.]
	\underline{Reduction to continuous approximations:} Since it is hard to show the desired result $\mu_h\circ\psi = \mu_{\tilde h}$ directly we will work with approximations $\mu_h^n$ to $\mu_h$ as given by theorem \ref{thm:approximationofhwithhn} (note that they are well-defined functions, as opposed to distributions) and show that $\mu_h^n\circ\psi=\mu_{\tilde h}^n$ for all $n\in\mathbb N$ and hence for their (weak) limit.
	
	\underline{Show it for the approximation:}
	\begin{align*}
		\mu_{\tilde h}^n&\eqbydef
		\exp\left(\gamma(h^n\circ\psi+Q\log|\psi'|)-\frac{\gamma^2}{2}\text{var}(h^n\circ\psi+Q\log|\psi'|)+\frac{\gamma^2}{2}\log C(z;D) \right)\\
		&=\exp\left(\gamma h^n\circ\psi+\frac{\gamma^2}{2}\log|\psi'|+2\log|\psi'|-\frac{\gamma^2}{2}\text{var}(h^n\circ\psi)+\frac{\gamma^2}{2}\log C(z;D) \right)\\
		&=\exp\left(\gamma h^n\circ\psi+\frac{\gamma^2}{2}\log(|\psi'|C(z;D))-\frac{\gamma^2}{2}\text{var}(h^n\circ\psi)\right)|\psi'|^2\\
		&=\mu_h^n\circ\psi,
	\end{align*}
	where the last step holds since \begin{itemize}
		\item $|\psi'(z)|=\frac{|\psi'(z)|C(z,\tilde D)}{C(z,\tilde D)}=\frac{C(\psi(z);D)}{C(z;\tilde D)}$, so the factor $\frac{\gamma}{2}\log|\psi'|$ is here to adjust the conformal radius, and
		\item $|\psi'(z)|^2 = |\text{det}(D\psi)(z)|$ is the factor we get when using the integration by substitution formula integrating against the Radon Nikodym derivative $\frac{d\mu^n}{dz}$ and going from $\tilde D$ to $D$ by $\psi$.
	\end{itemize}
\end{proof}

This proof relied heavily on the following theorem which is also of independent interest:

\begin{thm}\label{thm:approximationofhwithhn}
	Write $h\deq \overline h+h^0$ where $\overline h$ is the zero boundary Gaussian free field on some domain $D$ and $h^0$ is a deterministic continuous function on $D$. Let $(f_i)_{i\in\mathbb N}$ be an orthonormal basis of continuous functions for $H(D)$ and let $h^n\deq h^0+\sum_{i=1}^n (h,f_i)_\nabla f_i$. Then $\mu=\mu_h$ is almost surely the weak limit of $$\mu_h^n=\mu^n\deq exp\left(\gamma h^n(z)-\frac{\gamma^2}{2}\text{Var}h^n(z)+\frac{\gamma^2}{2}\log C(z;D) \right)dz$$
	as $n\rightarrow\infty$.
	
	Furthermore, for each measurable $A\subseteq D$ and for each $n\geq 0$ we have $$\mathbb E[\mu(A)|h^n]=\mu^n(A).$$
\end{thm}

Before we get to the proof of this theorem we define rooted random measures and record some of their properties which will be used later on.

\begin{defi}[Rooted Random Measures]
	Let $\Theta_\eps\deq Z^{-1}_\eps e^{\gamma h_\eps(z)}dzdh$, where $Z^{-1}_\eps$ is some normalising constant chosen such that $\Theta_\eps$ is a probability measure and $dh$ is the law of $h$.
\end{defi}
\begin{rem}[$\Theta_\eps$ is well-defined]
	By the Radon-Nikodym theorem $\Theta_\eps$ is well defined since the Radon-Nikodym derivative of $\theta_\eps$ with respect to $dzdh$ is $Z^{-1}_\eps e^{\gamma h_\eps(z)}$, a non-negative continuous (hence measurable) function on a measurable space $H\times D$ endowed with the product $\sigma$-algebra of their two respective $\sigma$-algebras and $dzdh$ and $\theta_\eps$ are both $\sigma$-finite measures.
\end{rem}

\begin{rem}[Marginal Laws and Conditional Distributions]
	\todo{do I really want to do the proof of this theorem? if not this section is for nothing}
\end{rem}

\begin{proof}[Proof of the theorem.]
	
\end{proof}

\section{Connection to SLEs}
Before we state the main result of this section we first have to fix notation and introduce some new objects we will be working with.
\subsection{Preliminaries}

\subsubsection{SLEs}
Naturally the object of central interest in this section is the SLE. In this subsection we shall merely fix notation.

\begin{defi}[Chordal $\text{SLE}_\kappa$ in $\mathbb H$ from $0$ to $\infty$]
	It is well known that for each $\kappa\geq 0$ and instance of a Brownian motion $B=(B_t)_{t\geq 0}$ there is almost surely a unique continuous curve $\eta$ in $\mathbb H$ from $0$ to $\infty$ (parameterized by $[0,\infty)$) for which
	\begin{equation}\label{eq:LoewnerFlow}
		dg_t(z)=\frac{2}{g_t(z)-\sqrt\kappa B_t}dt,
	\end{equation}
where $g_t:\mathbb H\setminus\eta([0,t])\rightarrow\mathbb H$ are the unique conformal maps satisfying the normalisation $\lim_{z\rightarrow\infty}|g_t(z)-z|=0$.

Equation \eqref{eq:LoewnerFlow} is usually called the Loewner flow.

The resulting (random) object $\eta$ is said to be an instance of chordal $\text{SLE}_\kappa$ in $\mathbb H$ from $0$ to $\infty$ with half-plane capacity $t$.

We also set $\eta_T\deq\eta([0,T])$ and denote by $K_T$ the complement of the unbounded component of $\mathbb H\setminus\eta_T$.
\end{defi}
$\text{SLE}_\kappa$ is known to exhibit different behaviour for different choices of $\kappa$ which is stated in the following proposition:
\begin{pro}[Properties of $\text{SLE}_\kappa$] We distinguish between the following cases:
	\begin{itemize}
		\item $\kappa\in[0,4]$: $\text{SLE}_\kappa$ is almost surely a simple curve.
		\item $\kappa\in(4,8)$: $\text{SLE}_\kappa$ is almost surely a self-intersecting, but non-space-filling curve.
		\item $\kappa\geq 8$: $\text{SLE}_\kappa$ is almost surely a space-filling curve.
	\end{itemize}
\end{pro}
So for $\kappa\leq 4$ we get that $K_T=\eta_T$ almost surely.

In what follows we will mainly deal with the denominator in\eqref{eq:LoewnerFlow} which we will henceforth refer to as $$f_t(z)\deq g_t(z)-\sqrt{\kappa}B_t,$$ satisfying $$df_t(z)=\frac{2}{f_t(z)}dt-\sqrt\kappa dB_t,$$ and $f_t(\eta(t))=0$.


\subsubsection{Free Boundary GFF}
\todo{add properties as they are needed in the proof (p37ff)}
% remark about how the +QlogBLA corresponds to recentering free boundary GFFs

\subsection{Sampling Quantum Surfaces with SLEs}
The next theorem provides another way of sampling a quantum surface $(\mathbb H,h)$ as defined in \ref{def:quantumsurface}, where $h\deq \tilde h+\frac{2}{\sqrt\kappa}\log|\cdot|$ and $\tilde h$ is a free GFF on $\mathbb H$. Informally it may be rephrased as:
The law of the quantum surface $(\mathbb H,h)$ is invariant under the following operation:
\begin{enumerate}
	\item sample a Brownian motion $(B_t)_{t\in[0,T]}$ inducing $f_T$ (indepedently of $h$)
	\item cut out $K_T$
	\item use $f_T^{-1}:\mathbb H\setminus K_T\rightarrow\mathbb H$ in the place of $\psi$ in definition \ref{def:quantumsurface} to get \begin{align*}
			f_T^{-1}(\mathbb H\setminus K_T, h)&\eqbydef (f_T^{-1}(\mathbb H\setminus K_T),h\circ f_T+Q\log|f'_T|))\\ &= (\mathbb H,h\circ f_T+Q\log|f'_T|))
		\end{align*}
\end{enumerate}
Note that the definition of a quantum surface gives us that even for deterministic $\psi$ we have that the induced surfaces are invariant under the $\psi^{-1}$ operation as in the third step above. The non-trivial part is that we can neglect the information in $K_T$ but ``regain'' it (in law) by the randomness of $K_T$. This cannot be possible for any deterministic $\psi$.

%\begin{rem}[Geometric Intuition]
%\end{rem}

% see that a GFF $h$ (as a (dual) GP) is characterised by its finite dimensional distributions since $(\eps,z)\mapsto h_\eps(z)$ is H\"older of BLA: <-- this is only needed to see that $h$ as a distribution uniquely determines the LQG measures $\mu_h$

%\begin{thm}[Continuity of $h_\eps(z)$]
	
%\end{thm}
%\begin{proof}
	
%\end{proof}

%we will now show the theorem by showing that the finite dimensional distributions coincide

While the following theorem can be shown for general $\kappa>0$ we will restrict ourselves to $\kappa\leq 4$ to avoid technical difficulties introduced by $K_T$ not being a curve anymore.

\begin{thm}
	Fix $\kappa\in (0,4]$ and let $\eta_T$ be the segment of $\text{SLE}_\kappa$ generated by a reverse Loewner flow $$df_t(z)=-\frac{2}{f_t(z)}dt-\sqrt\kappa dB_t, \;\;\;\; f_0(z)=z$$
	up to a fixed time $T>0$. Furthermore let \begin{itemize}
		\item $Q\deq\frac{2}{\sqrt\kappa}+\frac{\sqrt\kappa}{2}$ (as in proposition \ref{prop:changeofcoordinates} with $\gamma=\sqrt\kappa$),
		\item $\fh_0(z)\deq\frac{2}{\sqrt\kappa}\log|z|$,
		\item $\fh_t(z)\deq\fh_0(f_t(z))+Q\log|f'_t(z)|$,
		\item $\tilde h$ an instance of the free boundary Gaussian free field on $\mathbb H$, independent of $B=(B_t)_{t\geq 0}$.
	\end{itemize}
	Then the following two random distributions (modulo additive constants since $\tilde h$ is a \emph{free boundary} GFF) on $\mathbb H$ agree in law:
	\begin{align*}
		h&\deq \fh_0+\tilde h\\
		h\circ f_T+Q\log|f'_T|&\eqbydef\fh_T+\tilde h\circ f_T
	\end{align*}
\end{thm}
\begin{rem}
	First, we notice that the statement is that $h$ and $h\circ f_T+Q\log|f'_T|$ agree as distributions on $\mathbb H$. This is a non-trivial statement for two reasons:
	\begin{enumerate}
		\item Our definition of a Gaussian free field is not a random distribution, but a Gaussian process indexed by $H(D)$. However, as stated (without proof) in remark \ref{rem:spaceofX}, it turns out that the GFF (as a random distribution) is sufficiently nice to have a unique extension to the larger space of (non-continuous) functionals on $H(D)$, which is compatible with our definition of the GFF. Henceforth we shall use the definition of a GFF as random distribution, knowing that it can be shown to be equivalent to ours.
		\item By the conformal invariance of the GFF $h\circ f_T$ is a GFF on $\mathbb H\setminus K_T$, so a priori it is not a distribution on $\mathbb H$. \todo{how is this well-defined for a free boundary GFF? (in particular in the case kappa bigger than 4)}
	\end{enumerate}
\end{rem}
\begin{proof}
	Two deterministic distributions on $D$ are equal if their evaluations at every $\rho\in H_s(D)$ coincide. To show that the two random distributions $h\eqbydef \fh_0+\tilde h$ and $\fh_T+\tilde h\circ f_T$ are equal in distribution we will thus show that \begin{equation}\label{eq:equalityneededformainresultofSLEconnection}
			(\fh_0,\rho)+(\tilde h,\rho)\eqby{d}(\fh_T,\rho)+(\tilde h\circ f_T,\rho).
		\end{equation}
	
	Let $G_T$ be the Green's function of $\mathbb H\setminus K_T$ and $E_T\deq\int \rho(x)G_T(x,y)\rho(y)dxdy$, then we have that
	\begin{itemize}
		\item $(\fh_0,\rho)$ is deterministic.
		\item $(\tilde h,\rho)\sim\mathcal N(0,E_0)$.
		\item $(\tilde h\circ f_T,\rho)\sim\mathcal N(0,E_T)$ and it is independent of $(\fh_T,\rho)$ since $\tilde h$ was chosen independently of the Brownian motion inducing $f_T$.
	\end{itemize}
	Now assume we had shown that $X_T\deq(\fh_T,\rho)$ was a continuous local martingale with quadratic variation $[X]_T=E_0-E_T$. Then, by Dubins-Schwarz we would have that the recentered process $M_T\deq(\fh_T,\rho)-(\fh_0,\rho)$ (chosen such that $M_0=0$) satisfies $M_t=\tilde B_{E_0-E_T}$, where $\tilde B$ is a Brownian motion, or equivalently: $(\fh_T,\rho)\sim\mathcal N((\fh_0,\rho),E_0-E_T)$.
	
	As a result we could rewrite \eqref{eq:equalityneededformainresultofSLEconnection} as $$\mathcal N((\fh_0,\rho),E_0)\eqby{d}\mathcal N((\fh_0,\rho),E_0-E_T)+\mathcal N(0,E_T),$$
	keeping in mind that the two normals on the r.h.s. are independent so that their variances add up and we would get the desired result.
	
	Hence it suffices to prove the assumption that $(\fh_T,\rho)$ is a continuous local martingale with quadratic variation $E_0-E_T$. The strategy will be as follows:
	\begin{enumerate}
		\item Calculate $d\fh_T(z)$ to see that it is a continuous, local martingale.\footnote{At this point we might already expect that $(\fh_T,\rho)$, as a ``sufficiently nice'' linear combination, is one as well.}
		\item Show that $\fh_t(z)$ is actually a continuous (proper) martingale so that, by Doob, we can find that it is a well-defined distribution.
		\item This enables us to show that $\fh_t(z)\in L^p_\text{loc}$ and hence $(\fh_t(z),\rho)$ is continuous in $t$.
		\item Then we calculate $d[\fh_t(z),\fh_t(y)]$ and $d[(\fh_t(z),\rho),(\fh_t(z),\rho)]$ by applying the stochastic Fubini theorem - this implies that it is a (continuous) local martingale with the right quadratic variation.
	\end{enumerate}
	
\end{proof}

\section{Connection to the KPZ formula}

Since the proofs of the following statements are beyond the scope of a part III essay we will restrict ourselves to stating the very basic definitions and theorems to give an idea how the KPZ formula translates expected fractal dimensions of random subsets of the plane to fractal dimensions in terms of Liouville quantum gravity.

To define fractal dimensions with respect to a general measure $\mu$ we first have to define a ball with respect to $\mu$:
\begin{defi}[Isothermal Quantum Ball]
	For some fixed measure $\mu$ on the domain $D$ (henceforth this will be referred to as \emph{quantum measure}) let $B^\delta(z)\deq B_\eps(z)$, the Euclidean ball centered at $z\in D$ with radius $\eps\deq\sup\{\eps':\mu(B_{\eps'}(z)\leq\delta)\}$. Then $B^\delta(z)$ is called the \emph{isothermal quantum ball} of area $\delta$ centered at $z$.
\end{defi}
\begin{rem}
	In the above definition we would like to pick the Euclidean ball whose radius $\eps$ is chosen such that $\mu(B_\eps(z))=\delta$, but in general such a delta does not need to exist, which is the reason for the less intuitive definition with the supremum.
\end{rem}
\begin{rem}
	Note that if $\gamma=0$ then $\mu$ as in theorem \ref{thm:weaklimitofmueps} (the limit of $\mu_\eps\deq e^{\gamma\overline h_\eps}dz$) is just the Lebesgue measure and $\delta=\pi\eps^2$.
\end{rem}
Recall the definition of the $\eps$-neighbourhood of $X\subseteq D$ by $$B_\eps(X)\deq \{z:B_\eps(z)\cap X\neq\emptyset \}$$ which serves as a motivation for the following definition:
\begin{defi}[Isothermal Quantum $\delta$ Neighbourhood of $X$]
	$$B^\delta(X)\deq \{z:B^\delta(z)\cap X\neq\emptyset\}$$
\end{defi}

Fix $\gamma\in[0,2)$. We also recall that a random subset $X\subseteq D$ is said to have (Euclidean expectation) dimension $2(1-x)$, where $x$ is called the Euclidean scaling exponent, if the expected area of $B_\eps(X)$ decays like $(\eps^2)^x$. (Note that $D\subseteq\mathbb C\simeq\mathbb R^2$.) More precisely, $$x\deq \lim_{\eps\rightarrow 0}\frac{\log \mathbb E\mu_0(B_\eps(X))}{\log\eps^2},$$ where $\mu_0$ denotes the Lebesgue measure on $D$ and the expectation is taken over $X$.
\begin{rem}[Examples]
	Some easy examples for deterministic $X$ to illuminate the definition:
	\begin{itemize}
		\item $X=\{0\}$. We expect $x$ to be one since the dimension of a point is $0 = 2(1-1)$. Indeed we have $\mu_0(B_\eps(0))=\pi\eps^2$ so that $\frac{\log\eps^2+\log\pi}{\log\eps^2}\rightarrow 1=:x$.
		\item $X=[0,1]$. We expect $x$ to be $\frac{1}{2}$ since the dimension of a line is $1=2(1-\frac{1}{2})$. Indeed we have $\mu_0(B_\eps([0,1]))=O(\eps)$ so that $\frac{\log\eps+c}{\log\eps^2}\rightarrow 1/2=:x$.
	\end{itemize}
\end{rem}

This, in turn, motivates the definition of its ``quantum'' analogue:
\begin{defi}[Quantum Scaling Exponent]
	We say a random $X\subseteq D$ has quantum scaling exponent $\Delta$ if $$\lim_{\delta\rightarrow 0}\frac{\log\mathbb E\mu(B^\delta(X))}{\log\delta}=\Delta,$$
	where the expectation $\mathbb E$ is with respect to both $X$ and $\mu$ which are chosen independently.
\end{defi}

Now we can state (without proof) one of the central results of \cite{Dup10}:

\begin{thm}
	Fix $\gamma\in[0,2)$ and a compact subset $\tilde D\subseteq D$ of some domain $D$. If a random $X\cap\tilde D$ has Euclidean expected scaling exponent $x\geq 0$ then it has quantum scaling exponent $\Delta$, where $\Delta$ is the non-negative solution to $$x=\frac{\gamma^2}{4}\Delta^2+\left(1-\frac{\gamma^2}{4}\right)\Delta.$$
\end{thm}


\begin{thebibliography}{00}
\bibitem{She07} S. Sheffield: \emph{Gaussian free fields for mathematicians}, Arxiv: 0312099
\bibitem{Dup10} B. Duplantier and S. Sheffield: \emph{Liouville Quantum Gravity and KPZ}, Arxiv: 0808.1560v2
%\bibitem{HK14} L. Holst, T. Konstantopoulos: \emph{Runs in coin tossing: a general approach for deriving distributions for functionals}, Arxiv: 1407.6831
%\bibitem{AGZ10} G. Anderson, A. Guionnet, O. Zeitouni: \emph{An Introduction to Random Matrices}, Cambridge University Press, 2010
\end{thebibliography}


\end{document}